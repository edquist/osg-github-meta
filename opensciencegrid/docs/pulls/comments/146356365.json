{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/224#discussion_r146356365"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/224"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/146356365"
    }
  }, 
  "author_association": "OWNER", 
  "body": "\"for otherwise\"?", 
  "commit_id": "9732a87e8fa155eec0cb71b1cdd35072c5f0d934", 
  "created_at": "2017-10-23T18:39:03Z", 
  "diff_hunk": "@@ -1,235 +1,121 @@\n Site Planning\n =============\n \n-!!! warning\n-    This documentation is outdated and some of the listed technologies are no longer in use:\n-\n-    * GRAM and the Globus Gatekeeper have been removed in favor of HTCondor-CE\n-    * Managed Fork has been removed along with GRAM\n-    * GUMS and EDG-Mkgridmap are deprecated in favor of [LCMAPS-VOMS authentication](security/lcmaps-voms-authentication)\n-    * BeStMan is deprecated in favor of HDFS and load-balanced GridFTP\n-    * NFSLite has been removed\n-\n-    Please be patient while we update the document to reflect current procedures and technologies.\n-\n-\n-## About this Document\n-\n This document is for **System Administrators**. The purpose of the document is to provide an overview about the different ways to setup an OSG site and to encourage you to plan your site before you continue to install the OSG software on your site.\n \n After reading this document you should be able to identify the site elements needed to setup your OSG site and choose among different technology choices presented.\n \n-## Background\n-\n-The goal for the Open Science Grid software stack is to provide a uniform computing and storage interface across many independently managed computing and storage clusters. Scientists, researchers, and students, organized as virtual organizations (VOs), are the consumers of the CPU cycles and storage.\n-\n-Your site is encouraged to support as many OSG-registered VOs as possible, but you are not required to support all of them.\n-\n-As the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing and storage cluster available to and reliable for the VOs that you support. The OSG expects you to set up a gatekeeper node called a Compute Element (CE) on which the bulk of the OSG software gets installed. The end-user sends jobs into your cluster's batch system, your CE receives them and passes them out to Worker Nodes (WN) for execution. Some VOs and end-users require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in a Storage Element (SE). A site is not required to provide both a CE and an SE.\n-\n-## Site Policies\n-\n-OSG expects you to clearly specify your site's policies regarding resource access. Please write them on a web page, make this page part of your site registration, and make it available via the GOC publishing tool [MyOSG](http://my.opensciencegrid.org) and the OSG information management system, OIM. We encourage you to allow all virtual organizations registered with the OSG at least \"opportunistic use\" of your resources. You may need to preempt those jobs when higher priority jobs come around. The end-users using the OSG generally prefer having access to your site subject to preemption over having no access at all.\n-\n-## OSG Site Elements\n-\n-The OSG provides software and documentation to install and operate following services:\n-\n-\n-| Element               | Description                                                                                |\n-|:----------------------|:-------------------------------------------------------------------------------------------|\n-| Authorization Service | enables grid users to authorize with your site using their grid or voms proxies            |\n-| Compute Element       | enables grid users to run jobs on your site                                                |\n-| Worker Node Client    | enables grid jobs running on worker nodes to access grid tools                             |\n-| Storage Element       | enables grid users to store large amounts of data at your site                             |\n-| VO Management Service | provides functionality for VO Managers to manage the membership information of their users |\n-\n-\n-### Authorization Service\n-\n-Grid users will authorization with your site using their **grid or voms proxy**. The OSG provides two different services that let you control the authorization process:\n-\n-| **Service**                     | **Description**                                                           | **Advantages**               | **Disadvantages**                          |\n-|---------------------------------|---------------------------------------------------------------------------|------------------------------|--------------------------------------------|\n-| edg-mkgridmap                   | a simple program that contacts VOMS servers and creates a gridmap file    | easy to install and maintain | does not support voms proxies              |\n-| [GUMS](security/install-gums)   | a web service providing sophisticated controls of how users authorization | supports voms proxies        | requires Tomcat to be run as a web service |\n-\n-\n-!!! warning\n-    A **VOMS Server** is not an element of your site. Each **Virtual Organization** operates a central VOMS Server to manage membership information of its grid users. Please contact the **VO Manager** for your virtual organization to obtain more details.\n+Background\n+----------\n \n-### Compute Element\n+The goal for the OSG Software stack is to provide a uniform computing and storage fabric across many\n+independently-managed computing and storage resources. These individual services will be accessed by virtual\n+organizations (VOs), which will delegate the resources to scientists, researchers, and students.\n \n-A **Compute Element** allows grid users to run jobs on your site. It is software that provides following services when run on your **gatekeeper**: The standard installation is based on HTCondor-CE with RSV for monitoring.\n+_Sharing_ is a fundamental principle for the OSG: your site is encouraged to support as many OSG-registered VOs as\n+local conditions allow.  _Autonomy_ is another principle: you are not required to support any you do not want.\n \n-You must determine your security policy with regard to Unix ID management on the cluster. You may choose group accounts and/or dynamic accounts for all users.\n+As the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing\n+and storage resources available to and reliable for your support VOs. Fundamentally, there are three components:\n \n-You must choose the OS (Red Hat Enterprise Linux derivative), the batch system (Condor, PBS, LSF, SGE, and Slurm are presently supported), and the network architecture of your cluster. The default network assumption is public/private with NAT so you will need to advertise your architecture by changing some settings by hand if yours isn't like this. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster (NFS-lite).\n+- *Compute Element* (CE): This provides remote access for VOs to submit \"pilot jobs\" to your local batch system.\n+- *Worker node* (WN): A standardized runtime environment for pilot jobs, managed and allocated by the batch system.\n+- *Data*: Various services that provide access to the data and storage resources at your site.\n \n-The CE hosts information provider(s) and monitoring services, most of which are configured correctly by default. We require all OSG sites to deploy Gratia, the OSG accounting system. Your site thus sends accounting records to OSG about jobs run on your site and data transfers involving your site. Aggregated summaries of this information can be viewed via the [GRACC](https://gracc.opensciencegrid.org).\n-\n-\n-<table>\n-<thead>\n-<tr class=\"header\">\n-<th align=\"left\">Service</th>\n-<th align=\"left\">Description</th>\n-<th align=\"left\">Comments</th>\n-</tr>\n-</thead>\n-<tbody>\n-<tr class=\"odd\">\n-<td align=\"left\">HTCondor-CE</td>\n-<td align=\"left\">HTCondor-CE, based on the HTCondor batch system, provide a public entry point to your local batch system.<br />\n-It also allows grid users to <strong>fork</strong> jobs on your gatekeeper by default.</td>\n-<td align=\"left\">Required</td>\n-</tr>\n-<tr class=\"even\">\n-<td align=\"left\">RSV</td>\n-<td align=\"left\"><strong>R</strong>esource <strong>S</strong>ervice <strong>V</strong>alidation system which schedules execution of local &quot;probes&quot; of your CE (and SE), and reports the results up to the GOC. This is important for service availablity monitoring of OSG sites.</td>\n-<td align=\"left\">Required</td>\n-</tr>\n-<tr class=\"odd\">\n-<td align=\"left\">Squid</td>\n-<td align=\"left\">Squid is a caching proxy for the Web that enables restricted access of worker nodes to the web.</td>\n-<td align=\"left\">Optional</td>\n-</tr>\n-<tr class=\"even\">\n-<td align=\"left\">Syslog-ng</td>\n-<td align=\"left\">A logging service that can be used to forward CE logfiles to the GOC for troubleshooting purposes.</td>\n-<td align=\"left\">Optional</td>\n-</tr>\n-</tbody>\n-</table>\n-\n-\n-Additionally two shared file systems are for grid users to install applications **OSG\\_APP** (required) and to save data **OSG\\_DATA** (optional). If available, both must be mounted on the gatekeeper and all worker nodes:\n-\n-\n-| Shared Filesystem | Description                                  | Recommended Size                 | Typical Size\\[TB\\] | Comments                                                                    |\n-|:------------------|:---------------------------------------------|:---------------------------------|:------------------:|:----------------------------------------------------------------------------|\n-| HOME              | space for grid user home directories         | 10GB for each VO                 |      0.1 to 1      | required, auto-cleanup                                                      |\n-| OSG\\_APP          | space for grid users to install applications | 10GB for each VO                 |      0.1 to 1      | required, no auto-cleanup                                                   |\n-| OSG\\_DATA         | space for grid users to stage data           | 10GB for each VO and worker node |      0.1 to 10     | optional, no quota, auto-cleanup                                            |\n-| OSG\\_WN\\_TMP      | tmp space for users on worker nodes          | 2GB for each cpu core            |         0.1        | required, auto-cleanup                                                      |\n-| OSG\\_GRID         | location of the worker node client           | 10GB                             |         0.1        | optional/required (see %RED%NOTE%ENDCOLOR%) |\n+Depending on the VOs you want to support, data services may not be necessary.  Even a compute element is not strictly\n+necessary: OSG offers a \"hosted CE\" service where OSG will run the software provided only with a SSH connection to the\n+batch submit host.  Contact <mailto:goc@opensciencegrid.org> for more information on the hosted CE.\n \n+- The simplest way to support OSG is to only provide SSH logins for the hosted CE and install the worker node\n+  environment.\n+- Larger or more complex sites will elect to run their own CE in addition to the WN environment and the simplest\n+  data service (HTTP proxy cache).\n+- The largest sites will additionally run large-scale data services such as a \"storage element\".  This is often required\n+  for sites that want to support more complex organizations such as ATLAS or CMS.\n \n !!! note\n-    If you install wn-client on each node via RPM all the client software is available in the default path. There is no need for OSG\\_GRID. The RPM installation creates `/etc/osg/wn-client/` with dummy setup files for compatibility with old jobs looking for a OSG\\_GRID. New jobs should source the setup file in OSG\\_GRID if this is defined; if not, they should expect all the client binaries in the default PATH.\n-\n-\n-### Worker Node Client\n-\n-The [Worker Node Client](worker-node/install-wn) is software installed on each worker node to give programs running on the worker nodes access to grid utilities. While it is technically optional, it is **strongly recommended** that you install it on the gatekeeper and all worker nodes. This can be done as a local installation, in which the software is installed individually on every worker node, or as a shared installation in which the software is installed on one machine that shares it via a global file system to all the worker nodes. All the site configurations below are showing a local installation of the worker node client.\n-\n-### Storage Element\n-\n-A **Storage Element** provides grid users the possibility to read and write large amounts of data on your site using the **S**torage **R**esource **M**anager (**SRM**). All Storage Element implementations in the OSG support the gsiftp protocol and full or partial SRM specification. The selection of storage suitable for your site varies on anticipated usage patterns, available hardware, the choice of underlying distributed storage, support for a tape-archival backend etc.\n-\n-There are two types of storage element services provided by OSG which implement SRM v2. See pointers to instructions for these services:\n-\n--   [BeStMan](data/bestman-overview) - Sits in front of any POSIX filesystem. There is also a version which supports xrootd filesystems.\n--   [Hadoop](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallHadoop200SE) - Map-reduce based solution to aggregate off-the-shelf disks into a scalable reliable system.\n-\n-An SE must run correctly configured Grid Information Providers, Gratia accounting and RSV probes.\n-\n-These services are supported by the OSG Storage group. Please email <osg-storage@opensciencegrid.org> for installation and support questions for these services.\n-\n-| **Storage Requirements**                                                                     | **Min Hardware Requirements** | **OSG SE Solution**   |\n-|----------------------------------------------------------------------------------------------|-------------------------------|-----------------------|\n-| SRM interface, Dynamic Space Management Support                                              | Server with local disk        | BeStMan-fullmode      |\n-| SRM interface, No or Static Space Management Support                                         | Server with local disk or NFS | BeStMan-gateway       |\n-| SRM interface, No or Static Space Management Support, jobs need root protocol to access data | Multiple servers(&gt;3)       | BeStMan-gateway/Xrood |\n-| SRM interface, No or Static Space Management Support, file replication                       | Multiple servers(&gt;4)       | BeStMan-gateway/HDFS  |\n-| SRM interface, Dynamic Space Management Support, file replication, interface to tape backend | Multiple servers (&gt;5)      | dcache                |\n-\n-\n-### VO Management Service\n-\n-A **V**irtual **O**ganization **M**anagement **S**ervice (VOMS) controls who is a member of your VO. Each VO needs to provide one VOMS. Please contact the VO Manager of VO to find out about the VOMS of your VO.\n-\n-## Recommendations\n-\n-In this section we outline %RED%important%ENDCOLOR% recommendations for setting up an OSG site. The recommendations are based on the knowledge of experienced system administrators and will help you avoid typical problems operating an OSG site from the beginning.\n-\n-### Shared File Systems\n-\n-We recommend to use a dedicated server for hosting the shared file system. The expected load on the file server could be distributed further by providing a dedicated file server for **HOME** and **OSG\\_DATA** if possible. You should also consider to:\n-\n--   use a consistent mounting scheme for shared partitions when mounted on the gatekeeper with respect to all worker nodes\n--   use a reasonable automatic cleanup procedure for **HOME**, **OSG\\_DATA** and **OSG\\_WN\\_TMP**\n--   not use an automatic cleanup procedures for **OSG\\_APP**\n--   not use quota for **OSG\\_DATA**\n-\n-#### NFS Warning\n-\n-NFS is known to be an *easy* but less adequate for large sites. If you want to use NFS you should read the chapter *\"Optimizing NFS Performance\"* in the <http://nfs.sourceforge.net/nfs-howto/>. Software partitions that can be locally installed, such as OSG\\_GRID, should be locally installed and not shared unless you have an enterprise-class NFS server.\n-\n-### Compute and Storage Element\n-\n--   provide dedicated hardware for the Compute and the Storage Element\n--   use as many cpu cores and main memory as possible\n--   avoid running other grid services such as GUMS on the Compute and the Storage Element\n--   avoid running a file server on the Compute and the Storage Element\n-\n-### Use Job Manager managedfork instead of fork\n-\n-*GRAM* provides several jobmanagers that control how jobs will be executed on the gatekeeper and the worker nodes. By %RED%default jobmanager fork%ENDCOLOR% will be used to allow grid users to run maintenance jobs on the gatekeeper. Unfortunately fork doesn't provide a way to restrict the number of jobs that may be run simultaneously.\n-\n-By choosing %RED%managedfork instead of fork%ENDCOLOR% during the setup process, you will be able to restrict the number of simultaneous running jobs on the gatekeeper. We strongly recommend to choose managedfork as the default jobmanager when configuring the Compute Element.\n-\n-### Role based Authentication using VOMS Proxies\n-\n-*VOMS proxies* allow role based authentication. If your site will support VOMS proxies for authentication you will be able to restrict write access to shared file systems for grid users while providing write permissions to VO Administrators only. In this case we recommend to export **OSG\\_APP** read-only to the gatekeeper and all worker nodes for grid users and read-write for VO Managers only.\n-\n+    An essential concept on the OSG is the \"pilot job\".  The pilot, which arrives at your batch system, is sent by the\n+    VO and gets a resource allocation.  However, it _does not_ contain any scientific payload.  Once started, it will\n+    connect back to a resource pool and pull down individuals' scientific \"payload jobs\".  Hence, we do not think about\n+    submitting \"jobs\" to sites but rather \"resource requests\".\n+\n+Site Policies\n+-------------\n+\n+Sites are encouraged to clearly specify and communicate their local policies regarding resource access. One common\n+mechanism to do this is post them on a web page and make this page part of your site registration in\n+[MyOSG](http://my.opensciencegrid.org).  Written policies help external entities understand what your site wants to\n+accomplish with the OSG -- and are often internally clarifying.\n+\n+In line of our principle of *sharing*, we encourage you to allow virtual organizations registered with the OSG\n+\"opportunistic use\" of your resources. You may need to preempt those jobs when higher priority jobs come around.\n+The end-users using the OSG generally prefer having access to your site subject to preemption over having no access\n+at all.\n+\n+Compute Element\n+---------------\n+\n+A **Compute Element** allows VOs to access the resources at your site via external submission to a batch system.  To\n+provide this service, the OSG provides a vertically-integrated set of software centered around the \"HTCondor-CE\".\n+\n+As part of the CE deploy, you must make a few site design choices:\n+\n+- Host operating system: Red Hat Enterprise Linux 6 or 7 are supported, as well as derivative distributions (CentOS,\n+  ScientificLinux),\n+- The batch system:  HTCondor, PBS, LSF, SGE, and Slurm are presently supported.  About 70% of OSG sites select\n+  HTCondor.  Non-HTCondor sites will need a shared filesystem between the CE and worker nodes that is _writable by\n+  root_ from the CE.\n+- The network architecture of your cluster: Most VOs require unrestricted outbound network access from the worker nodes;\n+  this is typically done through a NAT.  For sites where this is difficult to provide, some VOs are be able to provide\n+  a whitelist of IP addresses they will need outbound access to; contact <mailto:goc@opensciencegrid.org> for support.\n+\n+The CE will map each VO to one Unix user accont at the site (due to historic concerns, a handful may map to 2-3\n+accounts).  Except in the case of HTCondor, the CE and the worker nodes need to have uniform username-to-UID mappings.\n+\n+In addition to the basic remote submission service, the CE providers accounting and monitoring services, which will\n+run by default when the CE is enabled and started. The OSG central accounting system, GRACC, recieves individual\n+records for each job run by the OSG on your CE. Aggregated summaries of this information can be viewed via the\n+[GRACC](https://gracc.opensciencegrid.org) web interface.\n+\n+Worker Node Client\n+------------------\n+\n+The Worker Node Client is software installed on each worker node to provide a minimal runtime environment for pilot\n+jobs.  We strive to keep this as lightweight as possible; it includes basic grid utilities, not scientific libraries.\n+\n+There is a wide range of worker node management practices; accordingly, we provide the worker node client in three\n+forms:\n+\n+- [*Classical RPM packaging*](worker-node/install-wn.md): Grid utilities are deployed at the system level using `yum`.\n+- [*Tarball installation*](worker-node/install-wn-tarball.md): Utilities are deployed onto a sitewide shared\n+  filesystem by unpacking a tarball and running a configuration script.\n+- [*CVMFS*](install-wn-oasis): CVMFS, a heavily-used global read-only filesystem, provides a copy of the grid utilities.\n+\n+Additionally, OSG packages a global read-only filesystem called [CVMFS](worker-node/install-cvmfs) for distribution of\n+software and data.  Many VOs either completely rely on CVMFS for software (LIGO, CMS, ATLAS) or rely on CVMFS for a\n+majority of jobs (OSG VO).  Some jobs can utilize worker nodes without CVMFS.  You will be able to share your\n+resources regardless, but installing CVMFS is a primary mechanism to attract more jobs.\n+\n+Data Service\n+------\n+\n+There are two types of data services on the OSG:\n+\n+- A *cache* pulls in data on read-demand, keeping it locally for further use.  We provide support for the\n+  [Squid HTTP](data/frontier-squid) cache for smaller working set sizes and software called [Xrootd](http://xrootd.org)\n+  for otherwise.  Almost every site on the OSG installs a Squid cache.", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/224#discussion_r146356365", 
  "id": 146356365, 
  "original_commit_id": "8f35c3f1e4750680a2b2a7c1354786b9724bb9bf", 
  "original_position": 276, 
  "path": "docs/site-planning.md", 
  "position": null, 
  "pull_request_review_id": 71285948, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/224", 
  "updated_at": "2017-10-26T02:21:30Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/146356365", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
