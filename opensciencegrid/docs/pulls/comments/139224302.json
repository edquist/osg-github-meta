{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/170#discussion_r139224302"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/170"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/139224302"
    }
  }, 
  "author_association": "OWNER", 
  "body": "Remove `!` before glideinWMS; `` =proxy_DN` `` should be `` `proxy_DN` ``", 
  "commit_id": "a5dbad173d6abc80444eee504b53370ee088f79b", 
  "created_at": "2017-09-15T18:50:20Z", 
  "diff_hunk": "@@ -0,0 +1,983 @@\n+GlideinWMS VO Frontend Installation\n+===================================\n+\n+About This Document\n+===================\n+\n+This document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG glidein factory. This software is the minimum requirement for a VO to use glideinWMS.\n+\n+This document assumes expertise with Condor and familiarity with the glideinWMS software. It **does not** cover anything but the simplest possible install. Please consult the [Glidein WMS reference documentation](http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/install.html) for advanced topics, including non-`root`, non-RPM-based installation.\n+\n+This document covers three components of the GlideinWMS a VO needs to install:\n+\n+-   **User Pool Collectors**: A set of `condor_collector` processes. Pilots submitted by the factory will join to one of these collectors to form a Condor pool.\n+-   **User Pool Schedd**: A `condor_schedd`. Users may submit Condor vanilla universe jobs to this schedd; it will run jobs in the Condor pool formed by the **User Pool Collectors**.\n+-   **Glidein Frontend**: The frontend will periodically query the **User Pool Schedd** to determine the desired number of running job slots. If necessary, it will request the factory to launch additional pilots.\n+\n+This guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs.\n+\n+![alt text][logo]\n+\n+[logo]: https://twiki.opensciencegrid.org/twiki/pub/Documentation/Release3/InstallGlideinWMSFrontend/simple_diagram.png \"GlideinWMS Simple Diagra\"\n+\n+Release\n+=======\n+\n+This document reflects glideinWMS v3.2.17.\n+\n+How to get Help?\n+================\n+\n+To get assistance about the OSG software please use [this page](../common/help.md).\n+\n+For specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support <glideinwms-support@fnal.gov>\n+\n+To request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to <osg-gfactory-support@physics.ucsd.edu> (see below).\n+\n+Requirements\n+============\n+\n+Host and OS\n+-----------\n+\n+1. A host to install the GlideinWMS Frontend (pristine node). \n+2. OS is Red Hat Enterprise Linux 6, 7, and variants (see [details](../common/yum.md)). Currently most of our testing has been done on Scientific Linux 6. \n+3. Root access\n+\n+The Glidein WMS VO Frontend has the following hardware requirements for a production host:\n+\n+- **CPU**: Four cores, preferably no more than 2 years old.\n+- **RAM**: 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.\n+- **Disk**: 30GB will be plenty sufficient for all the binaries, config and log files related to glideinWMS. As this will be an interactive submit host, plan enough disk space for your users' jobs. Depending on your workflow, this might require 2MB to 2GB per job in a workflow.\n+\n+Users\n+-----\n+\n+The Glidein WMS Frontend installation will create the following users unless they are already created.\n+\n+| User       | Default uid | Comment                                                                                                                        |\n+|:-----------|:------------|:-------------------------------------------------------------------------------------------------------------------------------|\n+| `apache`   | 48          | Runs httpd to provide the monitoring page (installed via dependencies).                                                        |\n+| `condor`   | none        | Condor user (installed via dependencies).                                                                                      |\n+| `frontend` | none        | This user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.      |\n+| `gratia`   | none        | Runs the Gratia probes to collect accounting data (optional see [the Gratia section below](#Adding_Gratia_Accounting_and_a_L)) |\n+\n+Note that if uid 48 is already taken but not used for the appropriate users, you will experience errors. [Details...](https://twiki.grid.iu.edu/bin/view/Documentation/Release3/KnownProblems#Reserved_user_ids_especially_for)\n+\n+Credentials and Proxies\n+-----------------------\n+\n+The VO Frontend will use two credentials in its interactions with the the other glideinWMS services. At this time, these will be proxy files. \n+\n+1. the %GREEN%VO Frontend proxy%ENDCOLOR% (used to authenticate with the other glideinWMS services). \n+2. one or more glideinWMS %RED%pilot proxies%ENDCOLOR% (used/delegated to the factory services and submitted on the glideinWMS pilot jobs).\n+\n+The %GREEN%VO Frontend proxy%ENDCOLOR% and the %RED% pilot proxy%ENDCOLOR% can be the same. By default, the VO Frontend will run as user `frontend` (UID is machine dependent) so these proxies must be owned by the user `frontend`.\n+\n+### VO Frontend proxy\n+\n+The use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the [proxy configuration section](#Proxy_Configuration). This can be a plain grid proxy (from `grid-proxy-init`), no VO extensions are required.\n+\n+**You must notify the Factory operation of the DN of this proxy when you initially setup the frontend and each time the DN changes**.\n+\n+### Pilot proxies\n+\n+This proxy is used by the factory to submit the glideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of this proxy (neither at the initial registration nor for subsequent changes). This second proxy has no special requirement or controls added by the factory but will probably require VO attributes because of the CEs: if you are able to use this proxy to submit jobs to the CEs where the Factory runs glideinWMS pilots for you, then the proxy is OK. You can test your proxy using `globusrun` or HTCondor-G\n+\n+To check the important information about a pem certificate you can use: `openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout`. You will need that to find out information for the configuration files and the request to the GlideinWMS factory.\n+\n+### Certificates/Proxies configuration example\n+\n+This document has a [proxy configuration section](#Proxy_Configuration) that uses the host certificate/key and a user certificate to generate the required proxies.\n+\n+| Certificate      | User that owns certificate | Path to certificate                                                           |\n+|:-----------------|:---------------------------|:------------------------------------------------------------------------------|\n+| Host certificate | `root`                     | `/etc/grid-security/hostcert.pem`            `/etc/grid-security/hostkey.pem` |\n+\n+[Here](../common/pki-cli.md) are instructions to request a host certificate.\n+\n+\n+Networking\n+----------\n+\n+For more details on overall Firewall configuration, please see our [Firewall documentation](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/FirewallInformation)\n+\n+| Service Name      | Protocol | Port Number      |Inbound|Outbound| Comment                 |\n+|:------------------|:---------|:-----------------|:------|:-------|-------------------------|\n+|HTCondor port range| tcp      |`LOWPORT, HIGHPORT`|`YES` |        |contiguous range of ports|\n+|GlideinWMS Frontend| tcp      | 9618 to 9660     |`YES`  |        |HTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)|\n+ \n+The VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Each running pilot requires 5 outgoing TCP ports. Incoming TCP ports 9618 to 9660 must be open.\n+\n+-   For example, 2000 running jobs require about 10,100 TCP connections. This will overwhelm many firewalls; if you are unfamiliar with your network topology, you may want to warn your network administrator.\n+\n+Before the installation\n+=======================\n+\n+Once all requirements are satisfied you must take a couple of actions before installing the Frontend:\n+\n+-   you need all the data to connect to a GWMS Factory\n+-   \\*Remember to install HTCondor BEFORE installing the Frontend ([instructions are below](#Install_HTCondor))\\*\n+\n+OSG Factory access\n+------------------\n+\n+Before installing the Glidein WMS VO Frontend you need the information about a [Glidein Factory](http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/factory/index.html) that you can access: \n+\n+1. (recommended) OSG is managing a factory at UCSD and one at GOC and you can request access to them \n+2. You have another Glidein Factory that you can access\n+3. You [install your own Glidein Factory](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallGlideinWMSFactory)\n+\n+To request access to the OSG Glidein Factory at UCSD you have to send an email to <osg-gfactory-support@physics.ucsd.edu> providing: \n+\n+1. Your Name \n+2. The VO that is utilizing the VO Frontend\n+3. The DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the [proxy configuration section](#Proxy_Configuration)) \n+4. You can propose a security name that will have to be confirmed/changed by the Factory managers (see below) \n+5. A list of sites where you want to run:\n+\n+- Your VO must be supported on those sites\n+- You can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers\n+- You can start with one single site\n+\n+In the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend \n+\n+1. The exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\". \n+2. The host of the Factory Collector: `gfactory-1.t2.ucsd.edu`\n+3. The DN os the factory, e.g. `/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu` \n+4. The factory identity, e.g.: `gfactory@gfactory-1.t2.ucsd.edu` \n+5. The identity on the factory you will be mapped to. Something like: `username@gfactory-1.t2.ucsd.edu` \n+6. Your security name. A unique name, usually containing your VO name: `My_SecName`\n+7. A string to add in the main factory query\\_expr in the frontend configuration, e.g. `stringListMember(\"%RED%VO%ENDCOLOR%\",GLIDEIN_Supported_VOs)`. From there you get the correct name of the VO (above in this list).\n+\n+Installation Procedure\n+======================\n+\n+Refer to installtion of the [InstallCertAuth](https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallCertAuth)\n+\n+Install HTCondor\n+----------------\n+\n+Most required software is installed from the Frontend RPM installation. HTCondor is the only exception since there are [many different ways to install it](CondorInformation), using the RPM system or not. You need to have HTCondor installed before installing the Glidein WMS Frontend. If yum cannot find a HTCondor RPM, it will install the dummy `empty-condor` RPM, assuming that you installed HTCondor using a tarball distribution.\n+\n+If you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:\n+\n+``` console\n+[root@client ~] # yum install condor.x86_64\n+# If you have a 32 bit host use instead:\n+[root@client ~] # yum install condor.i386\n+```\n+\n+See [this HTCondor document](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/CondorInformation) for more information on the different options.\n+\n+Download and install the VO Frontend RPM\n+----------------------------------------\n+\n+The RPM is available in the OSG repository:\n+\n+Install the RPM and dependencies (be prepared for a lot of dependencies).\n+\n+``` console\n+[root@client ~] # yum install glideinwms-vofrontend\n+```\n+\n+This will install the current production release verified and tested by OSG with default condor configuration. This command will install the glideinwms vofrontend, condor, the OSG client, and all the required dependencies all on one node.\n+\n+If you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:\n+\n+-   `yum install --enablerepo=osg-testing glideinwms-vofrontend`: The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.\n+-   `yum install --enablerepo=osg-contrib glideinwms-vofrontend`: The most recent development series release, ie version 3 release. This has newer features such as cloud submission support, but is less tested.\n+\n+Note that these commands will install default condor configurations with all services on one node.\n+\n+### Advanced: Multi-node Installation\n+\n+For advanced users requiring heavy usage on their submit node, you may want to consider splitting the usercollector, user submit, and vo frontend services.\n+\n+This can be doing using the following three commands (on different machines):\n+\n+``` console\n+[root@client ~] # yum install glideinwms-vofrontend-standalone\n+[root@client ~] # yum install glideinwms-usercollector\n+[root@client ~] # yum install glideinwms-userschedd\n+```\n+\n+In addition, you will need to perform the following steps:\n+\n+- On the vofrontend and userschedd, modify CONDOR\\_HOST to point to your usercollector. This is in `/etc/condor/config.d/00_gwms_general.config`. You can also override this value by placing it in a new config file. (For instance, `/etc/condor/config.d/99_local_custom.config` to avoid rpmsave/rpmnew conflicts on upgrades).\n+- In `/etc/condor/certs/condor_mapfile`, you will need to all DNs for each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the `glidecondor_addDN` to add these values.\n+- In the `/etc/gwms-frontend/frontend.xml` file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend xml are in the following sections.\n+\n+### Upgrade Procedure\n+\n+If you have a working installation of glideinwms-frontend you can just upgrade the frontend rpms and skip the most of the configuration procedure below. These general upgrade instructions apply when upgrading the glideinwms-frontend rpm within same major versions.\n+\n+``` console\n+%RED%# Update the glideinwms-vofrontend packages%ENDCOLOR%\n+[root@client ~] # yum update glideinwms\\*\n+%RED%# Update the scripts in the working directory to the latest one%ENDCOLOR%\n+%RED%# For RHEL 7, CentOS 7, and SL7%ENDCOLOR%\n+[root@client ~] # /usr/sbin/gwms-frontend upgrade\n+%RED%# For RHEL 6, CentOS 6, and SL6%ENDCOLOR%\n+[root@client ~] # service gwms-frontend upgrade\n+%RED%# Restart HTCondor because the configuration may be different%ENDCOLOR%\n+[root@client ~] # service condor restart\n+```\n+\n+!!! note\n+    The \\\\\\* on the yum update is important.**\n+\n+!!! warning\n+    When you do a generic yum update that will update also condor, the upgrade may restore the personal condor config file that you have to remove with `rm /etc/condor/config.d/00personal_condor.config`\n+\n+!!! note\n+    When upgrading to GlideinWMS 3.2.7 the second schedd is removed from the default configuration. For a smooth transition: \n+\n+    1. remove from **`/etc/gwms-frontend/frontend.xml`** the second schedd (the line containing **`schedd_jobs2@YOUR_HOST`**)\n+    2. reconfigure the frontend (`service gwms-frontend reconfig`)\n+    3. restart HTCondor (`service condor restart`)\n+\n+#### Upgrading glideinwms-frontend from v2 series to v3 series\n+\n+Due to incompatibilities between the major versions, upgrade process involves certain steps. Following instructions apply when upgrading glideinwms-frontend from a v2 series (example: v2.7.x) to a v3 series (v3.2.x)\n+\n+- Update the RPMs and backup configuration files\n+\n+``` console\n+%RED%# Stop the glideinwms-vofrontend service%ENDCOLOR%\n+[root@client ~] # service gwms-frontend stop\n+\n+%RED%# Backup the v2.7.x configuration%ENDCOLOR%\n+[root@client ~] # cp /var/lib/gwms-frontend/vofrontend/frontend.xml /var/lib/gwms-frontend/vofrontend/frontend-2.xml\n+[root@client ~] # cp /etc/gwms-frontend/frontend.xml /etc/gwms-frontend/frontend-2.xml\n+\n+%RED%# Update the glideinwms-vofrontend packages from v2.7.x to v3.2.x%ENDCOLOR%\n+[root@client ~] # yum update glideinwms\\*\n+```\n+\n+- Convert v2.7.x configuration to v3.2.x configuration (only for RHEL 6, CentOS 6, and SL6. RHEL5 and drivative are not supported by v3.2.x, RHEL7 and derivative were not supported by v2.7.x)\n+\n+``` console\n+[root@client ~] #  /usr/lib/python2.6/site-packages/glideinwms/frontend/tools/convert_frontend_2to3.sh -i /var/lib/gwms-frontend/vofrontend/frontend-2.xml -o /var/lib/gwms-frontend/vofrontend/frontend.xml -s /usr/lib/python2.6/site-packages/glideinwms\n+[root@client ~] #  /usr/lib/python2.6/site-packages/glideinwms/frontend/tools/convert_frontend_2to3.sh -i /etc/gwms-frontend/frontend-2.xml -o /etc/gwms-frontend/frontend.xml -s /usr/lib/python2.6/site-packages/glideinwms\n+```\n+\n+-   Update the scripts in the working directory\n+\n+``` console\n+%RED%# Update the scripts in the working directory to the latest one%ENDCOLOR%\n+[root@client ~] #  service gwms-frontend upgrade\n+```\n+\n+Configuration Procedure\n+=======================\n+\n+After installing the RPM, you need to configure the components of the glideinWMS VO Frontend:\n+\n+1.  Edit Frontend configuration options\n+2.  Edit Condor configuration options\n+3.  Create a Condor grid map file\n+4.  Reconfigure and Start frontend\n+\n+### Configuring the Frontend\n+\n+The VO Frontend configuration file is `/etc/gwms-frontend/frontend.xml`. The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.\n+\n+1. The VO you are affiliated with. This will identify those CEs that the glideinWMS pilot will be authorized to run on using the %RED%pilot proxy%ENDCOLOR% described previously in the this [section](#credentials-and-proxies). Sometimes the whole `query_expr` is provided to you by the factory (see Factory access above):\n+\n+        :::file\n+        <factory query_expr='((stringListMember(\"VO\", GLIDEIN_Supported_VOs)))'>\n+\n+2. Factory collector information. The `username` that you are assigned by the factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production factory, you will have to change also `DN`, `factory_identity` and `node` attributes. (refer to the information provided to you by the factory operator):\n+   \n+        :::file\n+        <collector DN=\"/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\" \n+                   comment=\"Define factory collector globally for simplicity\" \n+                   factory_identity=\"gfactory@gfactory-1.t2.ucsd.edu\" \n+                   my_identity=\"username@gfactory-1.t2.ucsd.edu\" \n+                   node=\"gfactory-1.t2.ucsd.edu\"/>\n+\n+3. Frontend security information.\n+\n+    - The `classad_proxy` in the security entry is the location of the VO Frontend proxy described previously [here](#credentials-and-proxies).\n+    - The `proxy_DN` is the DN of the `classad_proxy` above.\n+    - The `security_name` identifies this VO Frontend to the the Factory, It is provided by the factory operator.\n+    - The `absfname` in the credential (or proxy in v 2.x) entry is the location of the glideinWMS %RED%`pilot`%ENDCOLOR% proxy described in the requirements section [here](#credentials-and-proxies). There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources). ***The type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the factory. If there is no match, between these two attributes in one of the credentials and some entry in one of the factories, then this frontend cannot trigger glideins.***\n+Both the `classad_proxy` and `absfname` files should be owned by `frontend` user.\n+        \n+ \n+            :::file\n+            # These lines are from the configuration of v 3.x\n+            <security classad_proxy=\"/tmp/vo_proxy\" proxy_DN=\"DN of vo_proxy\" \n+                  proxy_selection_plugin=\"ProxyAll\" \n+                  security_name=\"The security name, this is used by factory\" \n+                  sym_key=\"aes_256_cbc\">\n+                  <credentials>\n+                    <credential absfname=\"/tmp/pilot_proxy\" security_class=\"frontend\" \n+                    trust_domain=\"OSG\" type=\"grid_proxy\"/>\n+                  </credentials>\n+            </security>\n+\n+4. The schedd information.\n+\n+    - The `DN` of the %GREEN%VO Frontend Proxy%ENDCOLOR% described previously [here](#credentials-and-proxies).\n+    - The `fullname` attribute is the fully qualified domain name of the host where you installed the VO Frontend (`hostname --fqdn`).\n+\n+    A secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the frontend to service requests from multiple submit hosts.\n+\n+\n+           <schedds>\n+             <schedd DN=\"Cert DN used by the schedd at fullname:\"\n+                   fullname=\"Hostname of the schedd\"/>\n+              <schedd DN=\"Cert DN used by the second Schedd at fullname:\"\n+                    fullname=\"schedd name@Hostname of second schedd\"/>\n+           </schedds>\n+\n+\n+5. The User Collector information.\n+\n+     - The `DN` of the %GREEN%VO Frontend Proxy%ENDCOLOR% described previously [here](#credentials-and-proxies).\n+     - The `node` attribute is the full hostname of the collectors (`hostname --fqdn`) and port\n+     - The `secondary` attribute indicates whether the element is for the primary or secondary collectors (True/False).\n+\n+     The default Condor configuration of the VO Frontend starts multiple Collector processes on the host (`/etc/condor/config.d/11_gwms_secondary_collectors.config`). The `DN` and `hostname` on the first line are the hostname and the host certificate of the VO Frontend. The `DN` and `hostname` on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector ports can be defined as a range, e.g., 9620-9660).\n+\n+            \n+            <collector DN=\"DN of main collector\" \n+                   node=\"hostname.domain.tld:9618\" secondary=\"False\"/>\n+            <collector DN=\"DN of secondary collectors (usually same as DN in line above)\" \n+                   node=\"hostname.domain.tld:9620-9660\" secondary=\"True\"/>\n+\n+!!! warning\n+    The Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):   \n+      \n+       - frontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)\n+       - work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)\n+\n+### If you have a different Factory\n+\n+The configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:\n+\n+1.  replace `gfactory@gfactory-1.t2.ucsd.edu` and `gfactory-1.t2.ucsd.edu` with the correct values for your factory. And control also that the name used for the frontend () matches.\n+2.  make sure that the factory is advertising the attributes used in the factory query expression (`query_expr`).\n+\n+Configuring Condor\n+------------------\n+\n+The condor configuration for the frontend is placed in `/etc/condor/config.d`. \n+\n+- 00_gwms_general.config \n+- 01_gwms_collectors.config \n+- 02_gwms_schedds.config \n+- 03_gwms_local.config\n+- 11_gwms_secondary_collectors.config\n+- 90_gwms_dns.config\n+\n+Get rid of the pre-loaded condor default to avoid conflicts in the configuration.\n+\n+    :::console\n+    [root@client ~] # rm /etc/condor/config.d/00personal_condor.config\n+\n+\n+For most installations create a new file named `/etc/condor/config.d/92_local_condor.config`\n+\n+### Using other Condor RPMs, e.g. UW Madison HTCondor RPM\n+\n+The above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using `yum list condor`. The version name should include \"osg\", e.g. `7.8.6-3.osg.el5`. \n+\n+If you are using the UW Madison Condor RPMS, be aware of the following changes:\n+\n+-   This Condor RPM uses a file `/etc/condor/condor_config.local` to add your local machine slot to the user pool.\n+-   If you want to disable this behavior (recommended), you should blank out that file or comment out the line in `/etc/condor/condor_config` for LOCAL\\_CONFIG\\_FILE. (Make sure that LOCAL\\_CONFIG\\_DIR is set to `/etc/condor/config.d`)\n+-   Note that the variable LOCAL\\_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.\n+\n+In general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files: \n+\n+    :::console\n+    [root@client ~] # condor_config_val -config \n+    Configuration source: \n+        /etc/condor/condor_config \n+    Local configuration sources: \n+        /etc/condor/config.d/00_gwms_general.config\n+        /etc/condor/config.d/01_gwms_collectors.config \n+        /etc/condor/config.d/02_gwms_schedds.config \n+        /etc/condor/config.d/03_gwms_local.config \n+        /etc/condor/config.d/11_gwms_secondary_collectors.config\n+        /etc/condor/config.d/90_gwms_dns.config \n+\t%RED%/etc/condor/condor_config.local%ENDCOLOR%\n+\n+If, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files.\n+\n+### Verify your Condor configuration\n+\n+1. The glideinWMS configuration files in `/etc/condor/config.d` should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.\n+\n+2. Verify the alll the expected HTCondor daemons are running: \n+\n+        :::console\n+        [root@client ~] # condor\\_config\\_val -verbose DAEMON\\_LIST DAEMON\\_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED\\_PORT, COLLECTOR0 \n+        COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12, \n+        COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23, \n+        COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, \n+        COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40\n+        Defined in '/etc/condor/config.d/11\\_gwms\\_secondary\\_collectors.config', line 193.\n+\n+If you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be ***no*** `startd` daemons listed.\n+\n+### Create a Condor grid mapfile.\n+\n+The Condor grid mapfile (`/etc/condor/certs/condor_mapfile`) is used for authentication between the glideinWMS pilot running on a remote worker node, and the local collector. Condor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of:\n+\n+- %RED%Each schedd proxy%ENDCOLOR%: The =DN` of each schedd that the frontend talks to. Specified in the frontend.xml schedd element `DN` attribute:\n+\n+        :::file\n+        <schedds>\n+          <schedd DN=\"/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\" fullname=\"YOUR_HOST\"/>\n+          <schedd DN=\"/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\" fullname=\"schedd_jobs2@YOUR_HOST\"/>\n+        </schedds>\n+\n+- %GREEN%Frontend proxy%ENDCOLOR%: The DN of the proxy that the frontend uses to communicate with the other !glideinWMS services. Specified in the frontend.xml security element =proxy_DN` attribute:", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/170#discussion_r139224302", 
  "id": 139224302, 
  "original_commit_id": "d5eda911e523206b2fba85b5a7a3e5c915eaffd3", 
  "original_position": 439, 
  "path": "docs/other/install-gwms-frontend.md", 
  "position": null, 
  "pull_request_review_id": 63127568, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/170", 
  "updated_at": "2017-09-15T20:07:27Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/139224302", 
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/5246893?v=4", 
    "events_url": "https://api.github.com/users/matyasselmeci/events{/privacy}", 
    "followers_url": "https://api.github.com/users/matyasselmeci/followers", 
    "following_url": "https://api.github.com/users/matyasselmeci/following{/other_user}", 
    "gists_url": "https://api.github.com/users/matyasselmeci/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/matyasselmeci", 
    "id": 5246893, 
    "login": "matyasselmeci", 
    "organizations_url": "https://api.github.com/users/matyasselmeci/orgs", 
    "received_events_url": "https://api.github.com/users/matyasselmeci/received_events", 
    "repos_url": "https://api.github.com/users/matyasselmeci/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/matyasselmeci/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/matyasselmeci/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/matyasselmeci"
  }
}
