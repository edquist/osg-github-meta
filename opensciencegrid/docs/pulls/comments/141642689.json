{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/181#discussion_r141642689"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/181"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/141642689"
    }
  }, 
  "author_association": "OWNER", 
  "body": "The contents of this section (except for the \"BeStManHadoop-specific configuration\" section and the first line of the \"Copy certificates to bestman location \" section) can be entirely replaced by a link to https://opensciencegrid.github.io/docs/data/bestman-install", 
  "commit_id": "8f4c7863db5102d1a58db004ce44134d73c4fe27", 
  "created_at": "2017-09-28T14:56:56Z", 
  "diff_hunk": "@@ -0,0 +1,1499 @@\n+**Hadoop 2.0.0 (CDH4)**\n+=======================\n+\n+<span class=\"twiki-macro DOC_STATUS_TABLE\"></span> \n+**Purpose**: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.\n+\n+!!! warning\n+    If you are installing Hadoop/Bestman from OSG 3.1, you will need to use [this](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallHadoopSE) guide instead. This guide details installing Hadoop 2.0 from the OSG 3.2 repositories.\n+\n+Preparation\n+===========\n+\n+Introduction\n+------------\n+\n+[Hadoop Distributed File System](http://hadoop.apache.org/hdfs/) (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:\n+\n+-   An [SRM interface](https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.html) for grid access;\n+-   GridFTP-HDFS as transport layer; and\n+-   A [FUSE interface](http://fuse.sourceforge.net/) for localized POSIX access.\n+-   [Apache Hadoop](http://hadoop.apache.org/)\n+\n+The OSG packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs and are available from the OSG repositories. It is also recommended that you enable [EPEL](http://fedoraproject.org/wiki/EPEL) repos.\n+\n+Note on upgrading from Hadoop 0.20\n+----------------------------------\n+\n+!!! note\n+    If upgrading, make sure to follow these instructions %RED%BEFORE%ENDCOLOR% any other instructions in this document.\n+\n+1. First, you must upgrade to the newest version of Hadoop-0.20. Older versions may have dependency and upgrade problems. Make sure that your version is at least `hadoop-0.20-0.20.2+737-26` (or newer) on all nodes. (The important number is the 26. Older release numbers may have upgrade problems). You may need to specify this version specifically to ensure that the correct version is installed. ie. `yum upgrade hadoop-0.20-0.20.2+737-26`.\n+1. Next, make sure all configuration and important files are backed up in case of catastrophic failure. In particular, backup a copy of `hdfs-site.xml`, `core-site.xml` and important namenode files.\n+1. Now, upgrade to hadoop-2.0.0 using `yum upgrade hadoop`\n+1. Also, make sure to bring in any new packages using the relevant meta-package, such as `yum install osg-se-hadoop-namenode`, `yum install osg-se-hadoop-datanode` or `yum install osg-se-hadoop-srm`.\n+1. On the namenode, run `hadoop namenode -upgrade` to upgrade the meta-data catalog.\n+1. Follow the configuration instructions below for each node. In particular, restore or modify `hdfs-site.xml` and `core-site.xml` then copy to all nodes. For any nodes using fuse mounts, note that \"hdfs\\#\" should be changed to \"hadoop-fuse-dfs\\#\" in `/etc/fstab`.\n+\n+Requirements\n+============\n+\n+Architecture\n+------------\n+\n+!!! note\n+    There are several important components to a storage element installation. Throughout this document, it will be stated which node the relevant installation instructions apply to. It can apply to one of the following:\n+\n+-   **Namenode**: You will have at least one namenode. The name node functions as the directory server and coordinator of the hadoop cluster. It houses all the meta-data for the hadoop cluster. %RED%The namenode and secondary namenode need to have a directory that they can both access on a shared filesystem so that they can exchange filesystem checkpoints.%ENDCOLOR%\n+-   **Secondary Namenode**: This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage. This dramatically improves startup and restart times.\n+-   **Datanode**: You will have many datanodes. Each data node stores large blocks of files to be stored on the hadoop cluster.\n+-   **Client**: This is a documentation shorthand that refers to any machine with the hadoop client commands and [FUSE](http://fuse.sourceforge.net/) mount. Any machine that needs a FUSE mount to access data in a POSIX-like fashion will need this.\n+-   **GridFTP node**: This is a node with [Globus GridFTP](http://dev.globus.org/wiki/GridFTP). The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration. You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.\n+-    **SRM node**: This node will contain the BeStMan SRM frontend for accessing the Hadoop cluster via the SRM protocol. [BeStMan2 SRM](https://sdm.lbl.gov/bestman/)\n+\n+Note that these components are not necessarily mutually exclusive. For instance, you may consider having your GridFTP server co-located on the SRM node. Alternatively, you can locate a client (or even a GridFTP node) co-located on each data node. That way, each data node also acts as an access point to the hadoop cluster.\n+\n+Please read the [planning document](https://twiki.opensciencegrid.org/bin/view/Documentation/HadoopUnderstanding) to understand different components of the system.\n+\n+!!! note\n+    Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email <osg-storage@opensciencegrid.org> and <osg-hadoop@opensciencegrid.org>.\n+\n+Host and OS\n+-----------\n+\n+Hadoop will run anywhere that Java is supported (including Solaris). However, these instructions are for RedHat derivants (including Scientific Linux) because of the RPM based installation. The current supported Operating Systems supported by the OSG are Red Hat Enterprise Linux 6, 7, and variants (see [details...](../release/supported_platforms)).\n+\n+\n+The HDFS prerequisites are:\n+\n+-   Minimum of 1 headnode (the namenode)\n+-   At least one node which will hold data, preferably at least 2. Most sites will have 20 to 200 datanodes.\n+-   Working Yum and RPM installation on every system.\n+-   `fuse` kernel module and `fuse-libs`.\n+-   Java RPM. If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency. Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.\n+\n+**Compatibility Note** Note that versions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.\n+\n+Users\n+-----\n+\n+This installation will create following users unless they are already created.\n+\n+| User      | Comment                                           |\n+|:----------|:--------------------------------------------------|\n+| `bestman` | Used by Bestman SRM server (needs sudo access).   |\n+| `hdfs`    | Used by Hadoop to store data blocks and meta-data |\n+\n+For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n+\n+For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n+\n+For gums users, this means that each user that can be authenticated by gums should be created on the server.\n+\n+Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n+\n+Certificates\n+------------\n+\n+<span class=\"twiki-macro STARTSECTION\">Certificates</span>\n+\n+| Certificate                 | User that owns certificate | Path to certificate                                                                                 |\n+|:----------------------------|:---------------------------|:----------------------------------------------------------------------------------------------------|\n+| Host certificate            | `root`                     | `/etc/grid-security/hostcert.pem` <br> `/etc/grid-security/hostkey.pem`                       |\n+| Bestman service certificate | `bestman`                  | `/etc/grid-security/bestman/bestmancert.pem` <br> `/etc/grid-security/bestman/bestmankey.pem` |\n+\n+<span class=\"twiki-macro ENDSECTION\">Certificates</span>\n+\n+[Instructions](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallCertScripts) to request a service certificate.\n+\n+You will also need a copy of CA certificates (see below). Note that the `osg-se-hadoop-srm` and `osg-se-hadoop-gridftp` package will automatically install a certificate package but will not necessarily pick the cert package you expect. For instance, certain installs will prefer the `osg-ca-scripts` package to fulfill this requirement, which installs a set of scripts to automatically update the certificates, but does not initialize the CA certs by default (you have to run it first). For this reason, you may want to specifically install the cert package of your choice first, before installing Hadoop.\n+\n+Networking\n+----------\n+\n+For more details on overall Firewall configuration, please see our [Firewall documentation](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/FirewallInformation).\n+\n+**NOTE:** The versions of Hadoop in OSG series 3.1 and 3.2 (ie, Hadoop 0.20 and 2.0.0) do not inter-operate. In order to use Hadoop 2.0.0, *all* nodes in the hadoop system (namenode, secondary namenode, datanodes, srm/gridftp nodes and all client nodes) must update to OSG 3.2 and Hadoop 2.0.0.\n+\n+Initializing Certificate Authority\n+==================================\n+\n+This is needed by GridFTP and SRM nodes, but it is recommended for all nodes in the cluster.\n+\n+++ Enable and Start `fetch-crl` To enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:\n+\n+``` console\n+%RED%# For RHEL 5, CentOS 5, and SL5 %ENDCOLOR%\n+root# /sbin/chkconfig fetch-crl3-boot on\n+root# /sbin/chkconfig fetch-crl3-cron on\n+%RED%# For RHEL 6, CentOS 6, and SL6, or OSG 3 _older_ than 3.1.15 %ENDCOLOR%\n+root# /sbin/chkconfig fetch-crl-boot on\n+root# /sbin/chkconfig fetch-crl-cron on\n+%RED%# For RHEL 7, CentOS 7, and SL7 %ENDCOLOR%\n+root# systemctl enable fetch-crl-boot\n+root# systemctl enable fetch-crl-cron\n+```\n+\n+To start fetch-crl:\n+\n+``` console\n+%RED%# For RHEL 5, CentOS 5, and SL5 %ENDCOLOR%\n+root# /sbin/service fetch-crl3-boot start\n+root# /sbin/service fetch-crl3-cron start\n+%RED%# For RHEL 6, CentOS 6, and SL6, or OSG 3 _older_ than 3.1.15 %ENDCOLOR%\n+root# /sbin/service fetch-crl-boot start\n+root# /sbin/service fetch-crl-cron start\n+%RED%# For RHEL 7, CentOS 7, and SL7 %ENDCOLOR%\n+root# systemctl start fetch-crl-boot\n+root# systemctl start fetch-crl-cron\n+```\n+\n+**NOTE**: while it is necessary to start `fetch-crl-cron` in order to have it active, `fetch-crl-boot` is started automatically at boot time if enabled. The start command will run `fetch-crl-boot` at the moment when it is invoked and it may take some time to complete.\n+\n+---%SHIFT%++ Configure `fetch-crl` To modify the times that fetch-crl-cron runs, edit `/etc/cron.d/fetch-crl` (or `/etc/cron.d/fetch-crl3` depending on the version you have).\n+\n+By default, `fetch-crl` connects directly to the remote CA; this is inefficient and potentially harmful if done simultaneously by many nodes (e.g. all the worker nodes of a big cluster). We recommend you provide a HTTP proxy (such as squid) the worker nodes can connect to. [Here](InstallFrontierSquid) are instructions to install a squid proxy.\n+\n+To configure fetch-crl to use an HTTP proxy server:\n+\n+-   If using `fetch-crl` version 2 (the `fetch-crl` package on RHEL5 only), then create the file `/etc/sysconfig/fetch-crl` and add the following line: <pre class=\"file\">\n+\n+export http\\_proxy=%RED%http://your.squid.fqdn:port<span class=\"twiki-macro ENDCOLOR\"></span> </pre> Adjust the URL appropriately for your proxy server.\n+\n+-   If using `fetch-crl` version 3 on RHEL5 via the `fetch-crl3` package or on RHEL6/RHEL7 via the `fetch-crl` package, then create or edit the file `/etc/fetch-crl3.conf` (RHEL5) or `/etc/fetch-crl.conf` (RHEL6/RHEL7) and add the following line: <pre class=\"file\">\n+\n+http\\_proxy=%RED%http://your.squid.fqdn:port<span class=\"twiki-macro ENDCOLOR\"></span> </pre> Again, adjust the URL appropriately for your proxy server.\n+\n+Note that the **`nosymlinks`** option in the configuration files refers to ignoring links within the certificates directory (e.g. two different names for the same file). It is perfectly fine if the path of the CA certificates directory itself (`infodir`) is a link to a directory.\n+\n+Any modifications to the configuration file will be preserved during an RPM update.\n+\n+For more details, please see our [fetch-crl documentation](../common/ca).\n+\n+Current versions of fetch-crl and fetch-crl3 produce more output. It is possible to send the output to syslog instead of the default email system. To do so:\n+\n+1.  Change the configuration file to enable syslog: <pre class=\"file\">\n+\n+logmode = syslog syslogfacility = daemon</pre>\n+\n+1.  Make sure the file `/var/log/daemon` exists, e.g. touching the file\n+2.  Change `/etc/logrotate.d` files to rotate it\n+\n+\n+Installation\n+============\n+\n+Installation depends on the node you are installing:\n+\n+Namenode Installation\n+---------------------\n+\n+``` console\n+root# yum install osg-se-hadoop-namenode\n+```\n+\n+Secondary Namenode Installation\n+-------------------------------\n+\n+``` console\n+root# yum install osg-se-hadoop-secondarynamenode\n+```\n+\n+Datanode Installation\n+---------------------\n+\n+``` console\n+root# yum install osg-se-hadoop-datanode\n+```\n+\n+Client/FUSE Installation\n+------------------------\n+\n+``` console\n+root# yum install osg-se-hadoop-client\n+```\n+\n+Standalone Gridftp Node Installation\n+------------------------------------\n+\n+``` console\n+root# yum install osg-se-hadoop-gridftp\n+```\n+\n+If you are using GUMS authorization, the follow rpms need to be installed as well:\n+\n+``` console\n+root# yum install lcmaps-plugins-gums-client\n+root# yum install lcmaps-plugins-basic\n+```\n+\n+SRM Node Installation\n+---------------------\n+\n+``` console\n+root# yum install osg-se-hadoop-srm\n+```\n+\n+!!! note\n+    If you are using a single system to host the SRM software and the gridftp node, you'll also need to install the `osg-se-hadoop-gridftp` rpm as well.\n+\n+Configuration\n+=============\n+\n+Hadoop Configuration\n+--------------------\n+\n+!!! note\n+    Needed by: Hadoop namenode, Hadoop datanodes, Hadoop client, GridFTP, SRM\n+\n+Hadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections.\n+\n+Hadoop configuration is stored in `/etc/hadoop/conf`. However, by default, these files are mostly blank. OSG provides a sample configuration in `/etc/hadoop/conf.osg` with most common values filled in. You will need to copy these into `/etc/hadoop/conf` before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify `hdfs-site.xml` and `core-site.xml`. Review all the settings in these files, but listed below are common settings to modify:\n+\n+|                 |                            |                                  |                                                                                           |\n+|-----------------|----------------------------|----------------------------------|-------------------------------------------------------------------------------------------|\n+| File            | Setting                    | Example                          | Comments                                                                                  |\n+| `core-site.xml` | fs.default.name            | hdfs://namenode.domain.tld.:9000 | This is the address of the namenode                                                       |\n+| `core-site.xml` | hadoop.tmp.dir             | /data/scratch                    | Scratch temp directory used by Hadoop                                                     |\n+| `core-site.xml` | hadoop.log.dir             | /var/log/hadoop-hdfs             | Log directory used by Hadoop                                                              |\n+| `core-site.xml` | dfs.umaskmode              | 002                              | umask for permissions used by default                                                     |\n+| `hdfs-site.xml` | dfs.block.size             | 134217728                        | Block size: 128MB by default                                                              |\n+| `hdfs-site.xml` | dfs.replication            | 2                                | Default replication factor. Generally the same as dfs.replication.min/max                 |\n+| `hdfs-site.xml` | dfs.datanode.du.reserved   | 100000000                        | How much free space hadoop will reserve for non-Hadoop usage                              |\n+| `hdfs-site.xml` | dfs.datanode.handler.count | 20                               | Number of server threads for datanodes. Increase if you have many more client connections |\n+| `hdfs-site.xml` | dfs.namenode.handler.count | 40                               | Number of server threads for namenodes. Increase if you need more connections             |\n+| `hdfs-site.xml` | dfs.http.address           | namenode.domain.tld.:50070       | Web address for dfs health monitoring page                                                |\n+\n+See <http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml> for more parameters to configure.\n+\n+!!! note\n+    Namenodes must have a `/etc/hosts_exclude` present\n+\n+### Special namenode instructions for brand new installs\n+\n+If this is a new installation (%RED%and only if this is a brand new installation<span class=\"twiki-macro ENDCOLOR\"></span>), you should run the following command as the `hdfs` user. (Otherwise, be sure to `chown` your storage directory to hdfs after running):\n+\n+``` console\n+hadoop namenode -format\n+```\n+\n+This will initialize the storage directory on your namenode\n+\n+FUSE Client Configuration\n+-------------------------\n+\n+!!! note\n+    Needed by: Hadoop client and SRM node. Recommended but not neccessary for GridFTP nodes.\n+\n+A FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"file system in user space\") is a way to access Hadoop using typical UNIX directory commands (ie POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available.\n+\n+FUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with `modprobe fuse`.\n+\n+You can add the FUSE to be mounted at boot time by adding the following line to `/etc/fstab`:\n+\n+``` file\n+hadoop-fuse-dfs# %RED%/mnt/hadoop%ENDCOLOR% fuse server=%RED%namenode.host%ENDCOLOR%,port=9000,rdbuffer=131072,allow_other 0 0\n+```\n+\n+Be sure to change the `/mnt/hadoop` mount point and `namenode.host` to match your local configuration. To match the help documents, we recommend using `/mnt/hadoop` as your mountpoint.\n+\n+Once your `/etc/fstab` is updated, to mount FUSE run:\n+\n+``` console\n+root# mkdir /mnt/hadoop\n+root# mount /mnt/hadoop\n+```\n+\n+When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:\n+\n+``` console\n+# mount /mnt/hadoop\n+INFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop\n+INFO fuse_options.c:110 Ignoring option allow_other\n+```\n+\n+If you have troubles mounting FUSE refer to [Running FUSE in Debug Mode](#running-fuse-in-debug-mode) in the Troubleshooting section.\n+\n+Creating VO and User Areas\n+--------------------------\n+\n+!!! note\n+    Grid Users are needed by GridFTP and SRM nodes. VO areas are common to all nodes.\n+\n+For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n+\n+For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n+\n+For gums users, this means that each user that can be authenticated by gums should be created on the server.\n+\n+Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n+\n+Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on **all nodes**. These are maintained in basic system files such as `/etc/passwd` and `/etc/group`.\n+\n+!!! note\n+    In the examples below It is assumed a FUSE mount is set to `/mnt/hadoop`. As an alternative `hadoop fs` commands could have been used.\n+\n+For clean HDFS operations and filesystem management:\n+\n+(a) Create top-level VO subdirectories under `/mnt/hadoop`.\n+\n+Example:\n+\n+``` console\n+root# mkdir /mnt/hadoop/cms\n+root# mkdir /mnt/hadoop/dzero\n+root# mkdir /mnt/hadoop/sbgrid\n+root# mkdir /mnt/hadoop/fermigrid\n+root# mkdir /mnt/hadoop/cmstest\n+root# mkdir /mnt/hadoop/osg\n+```\n+\n+(b) Create individual top-level user areas, under each VO area, as needed.\n+\n+``` console\n+root# mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina\n+root# mkdir -p /mnt/hadoop/cms/store/user/michaelthomas\n+root# mkdir -p /mnt/hadoop/cms/store/user/brianbockelman\n+root# mkdir -p /mnt/hadoop/cms/store/user/douglasstrain\n+root# mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana\n+```\n+\n+(c) Adjust username:group ownership of each area.\n+\n+``` console\n+root# chown -R cms:cms /mnt/hadoop/cms\n+root# chown -R sam:sam /mnt/hadoop/dzero\n+\n+root# chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas\n+```\n+\n+GridFTP Configuration\n+---------------------\n+\n+gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop. By now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as created the proper users/directories.\n+\n+The default settings in `/etc/gridftp.conf` along with `/etc/gridftp.d/gridftp-hdfs.conf` are used by the init.d script and should be ok for most installations. The file `/etc/gridftp-hdfs/gridftp-debug.conf` is used by `/usr/bin/gridftp-hdfs-standalone` for starting up the GridFTP server in a testing mode. Any additional config files under `/etc/gridftp.d` will be used for both the init.d and standalone GridFTP server. `/etc/sysconfig/gridftp-hdfs` contains additional site-specific environment variables that are used by the gridftp-hdfs dsi module in both the init.d and standalone GridFTP server. Some of the environment variables that can be used in `/etc/sysconfig/gridftp-hdfs` include:\n+\n+|                              |                |                                                                                                                                                                                                                                                                                 |\n+|------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| Option Name                  | Needs Editing? | Suggested value                                                                                                                                                                                                                                                                 |\n+| GRIDFTP\\_HDFS\\_REPLICA\\_MAP  | No             | File containing a list of paths and replica values for setting the default \\# of replicas for specific file paths                                                                                                                                                               |\n+| GRIDFTP\\_BUFFER\\_COUNT       | No             | The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop                                                                                                                                                                                     |\n+| GRIDFTP\\_FILE\\_BUFFER\\_COUNT | No             | The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop                                                                                                                                                                                 |\n+| GRIDFTP\\_SYSLOG              | No             | Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application)                                                                                                                                                            |\n+| GRIDFTP\\_HDFS\\_MOUNT\\_POINT  | Maybe          | The location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. **Note:** this does not imply you need FUSE mounted on GridFTP nodes! |\n+| GRIDFTP\\_LOAD\\_LIMIT         | No             | GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20.                                                                                                                                                          |\n+| TMPDIR                       | Maybe          | The temp directory where the file-based buffers are stored. Defaults to /tmp.                                                                                                                                                                                                   |\n+\n+`/etc/sysconfig/gridftp-hdfs` is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files (`ulimit -n`).\n+\n+Lastly, you will need to configure an authentication mechanism for GridFTP.\n+\n+### Configuring authentication\n+\n+For information on how to configure authentication for your GridFTP installation, please refer to the [configuring authentication section of the GridFTP guide](gridftp#configuring-authentication).\n+\n+GridFTP Gratia Transfer Probe Configuration\n+-------------------------------------------\n+\n+!!! note\n+    Needed by GridFTP node only.\n+\n+The Gratia probe requires the file `user-vo-map` to exist and be up to date. This file is created and updated by the `gums-client` package that comes in as a dependency of `osg-se-hadoop-gridftp` or `osg-gridftp-hdfs`. Assuming you installed GridFTP using the `osg-se-hadoop-gridftp` rpm, the Gratia Transfer Probe will already be installed.\n+\n+Here are the most relevant file and directory locations:\n+\n+|                     |                |                                          |\n+|---------------------|----------------|------------------------------------------|\n+| Purpose             | Needs Editing? | Location                                 |\n+| Probe Configuration | Yes            | /etc/gratia/gridftp-transfer/ProbeConfig |\n+| Probe Executables   | No             | /usr/share/gratia/gridftp-transfer       |\n+| Log files           | No             | /var/log/gratia                          |\n+| Temporary files     | No             | /var/lib/gratia/tmp                      |\n+| Gums configuration  | Yes            | /etc/gums/gums-client.properties         |\n+\n+The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file\n+\n+    /etc/gratia/gridftp-transfer/ProbeConfig\n+\n+This is usually one XML node spread over multiple lines. Note that comments (\\#) have no effect on this file. You will need to edit the following:\n+\n+|                                 |                                                                                            |                                                                                                                                            |\n+|---------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n+| Attribute                       | Needs Editing                                                                              | Value                                                                                                                                      |\n+| ProbeName                       | Maybe                                                                                      | This should be set to \"gridftp-transfer:<hostname>\", where <hostname> is the fully-qualified domain name of your gridftp host. |\n+| CollectorHost                   | Maybe                                                                                      | Set to the hostname and port of the central collector. By default it sends to the OSG collector. See below.                                |\n+| SiteName                        | Yes                                                                                        | Set to the resource group name of your site as registered in OIM.                                                                          |\n+| GridftpLogDir                   | Yes                                                                                        | Set to /var/log, or wherever your current gridftp logs are located                                                                         |\n+| Grid                            | Maybe                                                                                      | Set to \"ITB\" if this is a test resource; otherwise, leave as OSG.                                                                          |\n+| UserVOMapFile                   | No                                                                                         | This should be set to /var/lib/osg/user-vo-map; see below for information about this file.                                                 |\n+| SuppressUnknownVORecords| Maybe | Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended. |\n+| SuppressNoDNRecords             | Maybe                                                                                      | Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended.                                                     |\n+| EnableProbe                     | Yes                                                                                        | Set to 1 to enable the probe.                                                                                                              |\n+\n+### Selecting a collector host\n+\n+The collector is the central server which logs the GridFTP transfers into a database. There are usually three options:\n+\n+1. **OSG Transfer Collector**: This is the primary collector for transfers in the OSG. Use CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\".\n+1. **OSG-ITB Transfer Collector**: This is the test collector for transfers in the OSG. Use CollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\".\n+1. **Site local collector**: If your site has set up its own collector, then your admin will be able to give you an endpoint to use. Typically, this is along the lines of CollectorHost=\"collector.example.com:8880\".\n+\n+**Note:** if you are installing on an itb site, use **gratia-osg-itb.opensciencegrid.org** instead of \"gratia-osg-transfer.opensciencegrid.org\\* above.\n+\n+### Using GUMS authorization mode\n+\n+The `user-vo-map` file is a simple, space-separated format that contains 2 columns; the first is a unix username and the second is the VO which that username correspond to. In order to create it you need to configure the gums client.\n+\n+The primary configuration file for the gums-client utilities is located in `/etc/gums/gums-client.properties`. The two properties that you must change are:\n+\n+|               |               |                                                                                                                                                                                         |\n+|---------------|---------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| Attribute     | Needs Editing | Value                                                                                                                                                                                   |\n+| gums.location | Yes           | This should be set to the admin URL for your gums server, usually of the form gums.location=<https://GUMS_HOSTNAME:8443/gums/services/GUMSAdmin>                                        |\n+| gums.authz    | Yes           | This should be set to the authorization interface URL for your gums server, usually of the form gums.authz=<https://GUMS_HOSTNAME:8443/gums/services/GUMSXACMLAuthorizationServicePort> |\n+\n+After the gums client is configured to generate the file run the following once by hand:\n+\n+``` console\n+root# gums-host-cron\n+```\n+\n+`user-vo-map` should be created in the following location:\n+\n+    /var/lib/osg/user-vo-map\n+\n+To have cron regularly update this file start the following service:\n+\n+``` console\n+root# service gums-client-cron start\n+```\n+\n+Make sure the **UserVOMapFile** field is set to this location in\n+\n+    /etc/gratia/gridftp-transfer/ProbeConfig\n+\n+Without `user-vo-map` , all gridftp transfers will show up as belonging to the VO \"Unknown\".\n+\n+### Using Gridmap based authorization mode\n+\n+Note: If you are using this mode for authorization, make sure the files /etc/grid-security/gsi-authz.conf and /etc/grid-security/prima-authz.conf do not exist.\n+\n+In order to enable generation of grid-mapfile and osg-user-vo-map.txt by using the edg-mkgridmap cron process to get information form VOMS servers do the following:\n+\n+``` console\n+edg-mkgridmap \n+```\n+\n+If you have not installed this package, you will need to run `yum install edg-mkgridmap` first.\n+\n+\n+### Validation\n+\n+Run the Gratia probe once by hand to check for functionality:\n+\n+``` console\n+root# /usr/share/gratia/gridftp-transfer/GridftpTransferProbeDriver\n+```\n+\n+Look for any abnormal termination and report it if it is a non-trivial site issue. Look in the log files in `/var/log/gratia/<date>.log` and make sure there are no error messages printed.\n+\n+BeStMan Configuration\n+---------------------", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/181#discussion_r141642689", 
  "id": 141642689, 
  "original_commit_id": "f99ad729a9c507e9ede7001636d8e09ca20079aa", 
  "original_position": 502, 
  "path": "docs/data/install-hadoop-2-0-0.md", 
  "position": 413, 
  "pull_request_review_id": 65881447, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/181", 
  "updated_at": "2017-10-13T17:11:24Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/141642689", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
