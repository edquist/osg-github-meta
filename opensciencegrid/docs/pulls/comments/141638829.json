{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/181#discussion_r141638829"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/181"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/141638829"
    }
  }, 
  "author_association": "OWNER", 
  "body": "Turn this into ```!!! note```", 
  "commit_id": "8f4c7863db5102d1a58db004ce44134d73c4fe27", 
  "created_at": "2017-09-28T14:43:45Z", 
  "diff_hunk": "@@ -0,0 +1,1499 @@\n+**Hadoop 2.0.0 (CDH4)**\n+=======================\n+\n+<span class=\"twiki-macro DOC_STATUS_TABLE\"></span> \n+**Purpose**: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.\n+\n+!!! warning\n+    If you are installing Hadoop/Bestman from OSG 3.1, you will need to use [this](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallHadoopSE) guide instead. This guide details installing Hadoop 2.0 from the OSG 3.2 repositories.\n+\n+Preparation\n+===========\n+\n+Introduction\n+------------\n+\n+[Hadoop Distributed File System](http://hadoop.apache.org/hdfs/) (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:\n+\n+-   An [SRM interface](https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.html) for grid access;\n+-   GridFTP-HDFS as transport layer; and\n+-   A [FUSE interface](http://fuse.sourceforge.net/) for localized POSIX access.\n+-   [Apache Hadoop](http://hadoop.apache.org/)\n+\n+The OSG packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs and are available from the OSG repositories. It is also recommended that you enable [EPEL](http://fedoraproject.org/wiki/EPEL) repos.\n+\n+Note on upgrading from Hadoop 0.20\n+----------------------------------\n+\n+!!! note\n+    If upgrading, make sure to follow these instructions %RED%BEFORE%ENDCOLOR% any other instructions in this document.\n+\n+1. First, you must upgrade to the newest version of Hadoop-0.20. Older versions may have dependency and upgrade problems. Make sure that your version is at least `hadoop-0.20-0.20.2+737-26` (or newer) on all nodes. (The important number is the 26. Older release numbers may have upgrade problems). You may need to specify this version specifically to ensure that the correct version is installed. ie. `yum upgrade hadoop-0.20-0.20.2+737-26`.\n+1. Next, make sure all configuration and important files are backed up in case of catastrophic failure. In particular, backup a copy of `hdfs-site.xml`, `core-site.xml` and important namenode files.\n+1. Now, upgrade to hadoop-2.0.0 using `yum upgrade hadoop`\n+1. Also, make sure to bring in any new packages using the relevant meta-package, such as `yum install osg-se-hadoop-namenode`, `yum install osg-se-hadoop-datanode` or `yum install osg-se-hadoop-srm`.\n+1. On the namenode, run `hadoop namenode -upgrade` to upgrade the meta-data catalog.\n+1. Follow the configuration instructions below for each node. In particular, restore or modify `hdfs-site.xml` and `core-site.xml` then copy to all nodes. For any nodes using fuse mounts, note that \"hdfs\\#\" should be changed to \"hadoop-fuse-dfs\\#\" in `/etc/fstab`.\n+\n+Requirements\n+============\n+\n+Architecture\n+------------\n+\n+!!! note\n+    There are several important components to a storage element installation. Throughout this document, it will be stated which node the relevant installation instructions apply to. It can apply to one of the following:\n+\n+-   **Namenode**: You will have at least one namenode. The name node functions as the directory server and coordinator of the hadoop cluster. It houses all the meta-data for the hadoop cluster. %RED%The namenode and secondary namenode need to have a directory that they can both access on a shared filesystem so that they can exchange filesystem checkpoints.%ENDCOLOR%\n+-   **Secondary Namenode**: This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage. This dramatically improves startup and restart times.\n+-   **Datanode**: You will have many datanodes. Each data node stores large blocks of files to be stored on the hadoop cluster.\n+-   **Client**: This is a documentation shorthand that refers to any machine with the hadoop client commands and [FUSE](http://fuse.sourceforge.net/) mount. Any machine that needs a FUSE mount to access data in a POSIX-like fashion will need this.\n+-   **GridFTP node**: This is a node with [Globus GridFTP](http://dev.globus.org/wiki/GridFTP). The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration. You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.\n+-    **SRM node**: This node will contain the BeStMan SRM frontend for accessing the Hadoop cluster via the SRM protocol. [BeStMan2 SRM](https://sdm.lbl.gov/bestman/)\n+\n+Note that these components are not necessarily mutually exclusive. For instance, you may consider having your GridFTP server co-located on the SRM node. Alternatively, you can locate a client (or even a GridFTP node) co-located on each data node. That way, each data node also acts as an access point to the hadoop cluster.\n+\n+Please read the [planning document](https://twiki.opensciencegrid.org/bin/view/Documentation/HadoopUnderstanding) to understand different components of the system.\n+\n+!!! note\n+    Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email <osg-storage@opensciencegrid.org> and <osg-hadoop@opensciencegrid.org>.\n+\n+Host and OS\n+-----------\n+\n+Hadoop will run anywhere that Java is supported (including Solaris). However, these instructions are for RedHat derivants (including Scientific Linux) because of the RPM based installation. The current supported Operating Systems supported by the OSG are Red Hat Enterprise Linux 6, 7, and variants (see [details...](../release/supported_platforms)).\n+\n+\n+The HDFS prerequisites are:\n+\n+-   Minimum of 1 headnode (the namenode)\n+-   At least one node which will hold data, preferably at least 2. Most sites will have 20 to 200 datanodes.\n+-   Working Yum and RPM installation on every system.\n+-   `fuse` kernel module and `fuse-libs`.\n+-   Java RPM. If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency. Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.\n+\n+**Compatibility Note** Note that versions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/181#discussion_r141638829", 
  "id": 141638829, 
  "original_commit_id": "f99ad729a9c507e9ede7001636d8e09ca20079aa", 
  "original_position": 75, 
  "path": "docs/data/install-hadoop-2-0-0.md", 
  "position": null, 
  "pull_request_review_id": 65881447, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/181", 
  "updated_at": "2017-10-13T17:11:24Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/141638829", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
