{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/40#discussion_r138411170"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/40"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/138411170"
    }
  }, 
  "author_association": "OWNER", 
  "body": "A bunch of broken links", 
  "commit_id": "546819718bf9cccbbb2968b2ce9ab86b6da5713c", 
  "created_at": "2017-09-12T17:11:23Z", 
  "diff_hunk": "@@ -0,0 +1,550 @@\n+Configuration with OSG-Configure\n+================================\n+\n+-   [About this document](#about-this-document)\n+-   [Invocation and script usage](#invocation-and-script-usage)\n+-   [Syntax and layout](#syntax-and-layout)\n+-   [Configuration sections](#configuration-sections)\n+    -   Job managers (batch systems):\n+        -   [Bosco](#bosco)\n+        -   [Condor](#condor)\n+        -   [LSF](#lsf)\n+        -   [PBS](#pbs)\n+        -   [SGE](#sge)\n+        -   [Slurm](#slurm)\n+    -   Monitoring/reporting:\n+        -   [Gratia](#gratia)\n+        -   [Info Services](#info-services)\n+        -   [RSV](#rsv)\n+        -   [Subcluster / Resource Entry](#subcluster-resource-entry)\n+    -   [Gateway](#gateway)\n+    -   [Local Settings](#local-settings)\n+    -   [Misc Services](#misc-services)\n+    -   [Site Information](#site-information)\n+    -   [Squid](#squid)\n+    -   [Storage](#storage)\n+\n+\n+About this document\n+===================\n+\n+OSG-Configure and the INI files in `/etc/osg/config.d` allow a high level configuration of OSG services.\n+This document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.\n+\n+This page gives an overview of the options for each of the sections of the configuration files that `osg-configure` uses.\n+\n+\n+\n+Invocation and script usage\n+===========================\n+\n+The `osg-configure` script is used to process the INI files and apply changes to the system.\n+`osg-configure` must be run as root.\n+\n+The typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.\n+\n+To verify the config files, run:\n+``` console\n+[root@server] osg-configure -v\n+```\n+\n+OSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\n+Potential problems are:\n+\n+-   Required option not filled in\n+-   Invalid value\n+-   Syntax error\n+-   Inconsistencies between options\n+\n+To apply changes, run:\n+``` console\n+[root@server] osg-configure -c\n+```\n+\n+If your INI files do not change, then re-running `osg-configure -c` will result in the same configuration as when you ran it the last time.\n+This allows you to experiment with your settings without having to worry about messing up your system.\n+\n+OSG-Configure is split up into modules. Normally, all modules are run when calling `osg-configure`.\n+However, it is possible to run specific modules separately.\n+To see a list of modules, including whether they can be run separately, run:\n+``` console\n+[root@server] osg-configure -l\n+```\n+If the module can be run separately, specify it with the `-m %RED%<MODULE>%ENDCOLOR%` option:\n+``` console\n+[root@server] osg-configure -c -m %RED%<MODULE>%ENDCOLOR%\n+```\n+\n+Options may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\n+You may query the final value of an option via one of these methods:\n+``` console\n+[root@server] osg-configure -o %RED%<OPTION>%ENDCOLOR%\n+[root@server] osg-configure -o %RED%<SECTION>%ENDCOLOR%.%RED%<OPTION>%ENDCOLOR%\n+```\n+\n+Logs are written to `/var/log/osg/osg-configure.log`.\n+If something goes wrong, specify the `-d` flag to add more verbose output to `osg-configure.log`.\n+\n+The rest of this document will detail what to specify in the INI files.\n+\n+\n+Conventions\n+-----------\n+\n+In the tables below:\n+\n+-   Mandatory options for a section are given in **bold** type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.\n+-   Options that are not found in the default ini file are in *italics*.\n+\n+\n+\n+Syntax and layout\n+=================\n+\n+The configuration files used by `osg-configure` are the one supported by Python's [SafeConfigParser](http://docs.python.org/library/configparser.html), similar in format to the [INI configuration file](http://en.wikipedia.org/wiki/INI_file) used by MS Windows:\n+\n+-   Config files are separated into sections, specified by a section name in square brackets (e.g. `[Section 1]`)\n+-   Options should be set using `name = value` pairs\n+-   Lines that begin with `;` or `#` are comments\n+-   Long lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in [email RFC 822](http://tools.ietf.org/html/rfc822.html))\n+-   Variable substitutions are supported -- [see below](#variable-substitution)\n+\n+`osg-configure` reads and uses all of the files in `/etc/osg/config.d` that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called `[PBS]` can be given in `20-pbs.ini` as well as `99-local-site-settings.ini`.\n+\n+Each of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g. `99-local-site-settings.ini`) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.\n+\n+\n+Variable substitution\n+---------------------\n+\n+The osg-configure parser allows variables to be defined and used in the configuration file:\n+any option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name `myoption` in the section, you can substitute the value of that option elsewhere in the section by referring to it as `%(myoption)s`.\n+\n+!!! note\n+    The trailing `s` is required. Also, option names cannot have a variable subsitution in them.\n+\n+!!! warning\n+    You will need to be careful when naming variables in order to avoid an\n+    infinite loop when resolving the variable substitution.\n+\n+Special Settings\n+----------------\n+\n+If a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.\n+\n+### Ignore setting\n+\n+The `enabled` option, specifying whether a service is enabled or not, is a boolean but also accepts `Ignore` as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using `False` (or the `%(disabled)s` variable), because using `False` results in the service associated with the section being disabled. `osg-configure` will not change the configuration of the service if the `enabled` is set to `Ignore`.\n+\n+This is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the `Ignore` setting so that `osg-configure` does not alter the service's configuration and status.\n+\n+\n+\n+\n+Configuration sections\n+======================\n+\n+The OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g. `[Section 1]`). The configuration is split in multiple files and options form one section can be in more than one files.\n+\n+The following sections give an overview of the options for each of the sections of the configuration files that `osg-configure` uses.\n+\n+\n+\n+Bosco\n+-----\n+\n+This section is contained in `/etc/osg/config.d/20-bosco.ini` which is provided by the `osg-configure-bosco` RPM.\n+\n+| Option       | Values Accepted           | Explanation                                                                                                                                                                                                                                                                                                                   |\n+|--------------|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**  | `True`, `False`, `Ignore` | This indicates whether the Bosco jobmanager is being used or not.                                                                                                                                                                                                                                                             |\n+| **users**    | String                    | A comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here. |\n+| **endpoint** | String                    | The remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of <user@example.com>, exactly as you would use to ssh into the remote cluster.                                                                                                                                     |\n+| **batch**    | String                    | The type of scheduler installed on the remote cluster.                                                                                                                                                                                                                                                                        |\n+| **ssh\\_key** | String                    | The location of the ssh key, as created above.                                                                                                                                                                                                                                                                                |\n+\n+\n+Condor\n+------\n+\n+This section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the `enabled` setting should be set to `False`.\n+\n+This section is contained in `/etc/osg/config.d/20-condor.ini` which is provided by the `osg-configure-condor` RPM.\n+\n+| Option            | Values Accepted           | Explanation                                                                                                                                                                                                                                                                                                                            |\n+|-------------------|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**       | `True`, `False`, `Ignore` | This indicates whether the Condor jobmanager is being used or not.                                                                                                                                                                                                                                                                     |\n+| condor\\_location  | String                    | This should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the `osg-configure` script will try to get this from the CONDOR\\_LOCATION environment variable if available otherwise it will use `/usr` which works for the RPM installation.                               |\n+| condor\\_config    | String                    | This should be set to be path where the condor\\_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the `osg-configure` script will try to get this from the CONDOR\\_CONFIG environment variable if available otherwise it will use `/etc/condor/condor_config`, the default for the RPM installation. |\n+\n+\n+LSF\n+---\n+\n+This section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the `enabled` setting should be set to `False`.\n+\n+This section is contained in `/etc/osg/config.d/20-lsf.ini` which is provided by the `osg-configure-lsf` RPM.\n+\n+| Option             | Values Accepted           | Explanation                                                     |\n+|--------------------|---------------------------|-----------------------------------------------------------------|\n+| **enabled**        | `True`, `False`, `Ignore` | This indicates whether the LSF jobmanager is being used or not. |\n+| lsf\\_location      | String                    | This should be set to be directory where lsf is installed       |\n+\n+\n+PBS\n+---\n+\n+This section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the `enabled` setting should be set to `False`.\n+\n+This section is contained in `/etc/osg/config.d/20-pbs.ini` which is provided by the `osg-configure-pbs` RPM.\n+\n+| Option                         | Values Accepted           | Explanation                                                                                                                               |\n+|--------------------------------|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**                    | `True`, `False`, `Ignore` | This indicates whether the PBS jobmanager is being used or not.                                                                           |\n+| pbs\\_location                  | String                    | This should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs\\_location/bin. |\n+| **accounting\\_log\\_directory** | String                    | This setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.                    |\n+| pbs\\_server                    | String                    | This setting is optional and should point to your PBS server node if it is different from your OSG CE                                     |\n+\n+\n+SGE\n+---\n+\n+This section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the `enabled` setting should be set to `False`.\n+\n+This section is contained in `/etc/osg/config.d/20-sge.ini` which is provided by the `osg-configure-sge` RPM.\n+\n+| Option            | Values Accepted           | Explanation                                                                                                                                                            |\n+|-------------------|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**       | `True`, `False`, `Ignore` | This indicates whether the SGE jobmanager is being used or not.                                                                                                        |\n+| **sge\\_root**     | String                    | This should be set to be directory where sge is installed (e.g. same as **$SGE\\_ROOT** variable).                                                                      |\n+| **sge\\_cell**     | String                    | The sge\\_cell setting should be set to the value of $SGE\\_CELL for your SGE install.                                                                                   |\n+| default\\_queue    | String                    | This setting determines queue that jobs should be placed in if the job description does not specify a queue.                                                           |\n+| available\\_queues | String                    | This setting indicates which queues are available on the cluster and should be used for validation when `validate_queues` is set.                                      |\n+| validate\\_queues  | String                    | This setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note. |\n+\n+!!! note\n+    **validate_queues**:<br/>\n+    If `available_queues` is set, that list of queues will be used for\n+    validation, otherwise SGE will be queried for available queues.\n+\n+\n+Slurm\n+-----\n+\n+This section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the `enabled` setting should be set to `False`.\n+\n+This section is contained in `/etc/osg/config.d/20-slurm.ini` which is provided by the `osg-configure-slurm` RPM.\n+\n+| Option              | Values Accepted           | Explanation                                                                                                                                       |\n+|---------------------|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**         | `True`, `False`, `Ignore` | This indicates whether the Slurm jobmanager is being used or not.                                                                                 |\n+| **slurm\\_location** | String                    | This should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm\\_location/bin.    |\n+| db\\_host            | String                    | Hostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.                               |\n+| db\\_port            | String                    | Port of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.                                    |\n+| db\\_user            | String                    | Username used to access the SLURM database. This information is needed to configure the SLURM gratia probe.                                       |\n+| db\\_pass            | String                    | The location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe. |\n+| db\\_name            | String                    | Name of the SLURM database. This information is needed to configure the SLURM gratia probe.                                                       |\n+| slurm\\_cluster      | String                    | The name of the Slurm cluster                                                                                                                     |\n+\n+\n+Gratia\n+------\n+\n+This section configures Gratia. If `probes` is set to `UNAVAILABLE`, then `osg-configure` will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes, `%(osg-jobmanager-gratia)s`, `%(osg-gridftp-gratia)s`, `%(osg-metric-gratia)s`, `%(itb-jobmanager-gratia)s`, `%(itb-gridftp-gratia)s`, `%(itb-metric-gratia)s` are defined in the default configuration files to make it easier to specify the standard osg reporting.\n+\n+This section is contained in `/etc/osg/config.d/30-gratia.ini` which is provided by the `osg-configure-gratia` RPM.\n+\n+| Option       | Values Accepted            | Explanation                                                                                                                           |\n+|--------------|----------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**  | `True` , `False`, `Ignore` | This should be set to True if gratia should be configured and enabled on the installation being configured.                           |\n+| **resource** | String                     | This should be set to the resource name as given in the OIM registration                                                              |\n+| **probes**   | String                     | This should be set to the gratia probes that should be enabled. A probe is specified by using as `[probe_type]:server:port`. See note |\n+\n+!!! note\n+    **probes**:<br/>\n+    Legal values for `probe_type` are:\n+\n+    -   `metric` (for RSV)\n+    -   `jobmanager` (for the appropriate jobmanager probe)\n+    -   `gridftp` (for the GridFTP transfer probe)\n+\n+\n+Info Services\n+-------------\n+\n+Reporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.\n+\n+This section is contained in `/etc/osg/config.d/30-infoservices.ini`, which is provided by the `osg-configure-infoservices` RPM. (This is for historical reasons.)\n+\n+| Option        | Values Accepted           | Explanation                                                       |\n+|---------------|---------------------------|-------------------------------------------------------------------|\n+| **enabled**   | `True`, `False`, `Ignore` | True if reporting should be configured and enabled                |\n+| ce_collectors | String                    | The server(s) HTCondor-CE information should be sent to. See note |\n+\n+!!! note\n+    **ce_collectors**:\n+\n+    -   Set this to `DEFAULT` to report to the OSG Production or ITB servers (depending on your [Site Information](#site-information) configuration).\n+    -   Set this to `PRODUCTION` to report to the OSG Production servers\n+    -   Set this to `ITB` to report to the OSG ITB servers\n+    -   Otherwise, set this to the `hostname:port` of a host running a `condor-ce-collector` daemon\n+\n+\n+RSV\n+---\n+\n+This section handles the configuration and setup of the RSV services.\n+\n+This section is contained in `/etc/osg/config.d/30-rsv.ini` which is provided by the `osg-configure-rsv` RPM.\n+\n+| Option               | Values Accepted           | Explanation                                                                                                                                                                                                                                                            |\n+|----------------------|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **enabled**          | `True`, `False`, `Ignore` | This indicates whether the rsv  service is being used or not.                                                                                                                                                                                                          |\n+| **rsv_user**         | String                    | This gives username that rsv will run under.  If this is blank or set to `UNAVAILABLE`, it will default to rsv.                                                                                                                                                        |\n+| **gratia_probes**    | String                    | This settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer                                                      |\n+| ce_hosts             | String                    | This option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs (`hostname[:port/service]`) separated by a comma (e.g. `my.host,my.host2,my.host3:2812`).                                                   |\n+| htcondor_ce_hosts    | String                    | This option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs (`hostname[:port/service]`) separated by a comma (e.g. `my.host,my.host2,my.host3:2812`). |                           |\n+| gums_hosts           | String                    | This option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of *CE* or *SE* FQDNs (and _not a GUMS server FQDN_) separated by a comma (e.g. `my.host,my.host2,my.host3`). |\n+| gridftp_hosts        | String                    | This option lists the serviceURI of the gridftp servers that the RSV gridftp probes should check.  This should be a list of serviceURIs (`hostname[:port/service]`) separated by a comma (e.g. `my.host.iu.edu:2812,my.host2,my.host3`).                               |\n+| gridftp_dir          | String                    | This should be the directory that the gridftp probes should use during testing.  This defaults to `/tmp` if left blank or set to `UNAVAILABLE`.                                                                                                                        |\n+| **srm_hosts**        | String                    | This option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs (`hostname[:port/service]`) separated by a comma (e.g. `my.host,my.host2,my.host3:8444`).                                              |\n+| srm_dir              | String                    | This should be the directory that the srm probes should use during testing.                                                                                                                                                                                            |\n+| srm_webservice_path  | String                    | This option gives the webservice path that SRM probes need to use along with the host:port. See note.                                                                                                                                                                  |\n+| service_cert         | String                    | This option should point to the public key file (pem) for your service  certificate. If this is left blank or set to `UNAVAILABLE` and the `user_proxy` setting is set, it will default to `/etc/grid-security/rsvcert.pem`                                            |\n+| service_key          | String                    | This option should point to the private key file (pem) for your service  certificate. If this is left blank or set to `UNAVAILABLE` and the `service_cert` setting is enabled, it will default to `/etc/grid-security/rsvkey.pem` .                                    |\n+| service_proxy        | String                    | This should point to the location of the rsv proxy file. If this is left blank or set to `UNAVAILABLE` and the use_service_cert  setting is enabled, it will default to `/tmp/rsvproxy`.                                                                               |\n+| user_proxy           | String                    | If you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to `UNAVAILABE` or `DEFAULT`.   |\n+| **enable_gratia**    | `True`, `False`           | This option will enable RSV record uploading to central RSV collector at the GOC.   This should be set to True on all OSG resources (and to False on non-OSG resources).                                                                                               |\n+| **setup_rsv_nagios** | `True`, `False`           | This option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.<br> This plugin is provided as an experimental component, and admins are recommend *not to enable* it on production resources.        |\n+| rsv_nagios_conf_file | String                    | This option indicates the location of the rsv nagios  file to use for configuration details. This file *needs to be configured locally for RSV-Nagios forwarding to work* -- see inline comments in file for more information.                                         |\n+| condor_location      | String                    | If you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.                                                                                                     |\n+\n+!!! note\n+    **srm_webservice_path**:<br/>\n+    For dcache installations, this should work if left blank. However\n+    Bestman-xrootd SEs normally use `srm/v2/server` as web service path, and so\n+    Bestman-xrootd admins will have to pass this option with the appropriate\n+    value (for example: `srm/v2/server`) for the SRM probes to pass on their\n+    SE.\n+\n+\n+Subcluster / Resource Entry\n+---------------------------\n+\n+Subcluster and Resource Entry configuration is for reporting about the worker resources on your site. A **subcluster** is a homogeneous set of worker node hardware; a **resource** is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.\n+\n+**At least one Subcluster or Resource Entry section** is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.\n+\n+This section is contained in `/etc/osg/config.d/30-gip.ini` which is provided by the `osg-configure-gip` RPM. (This is for historical reasons.)\n+\n+This configuration uses multiple sections of the OSG configuration files:\n+\n+-   [Subcluster\\*](#subcluster-configuration): options about homogeneous subclusters\n+-   [Resource Entry\\*](#resource-entry-configuration-atlas-only): options for specifying ATLAS queues for AGIS\n+\n+### Notes for multi-CE sites.\n+\n+If you would like to properly advertise multiple CEs per cluster, make sure that you:\n+\n+-   Set the value of site\\_name in the \"Site Information\" section to be the same for each CE.\n+-   Have the **exact** same configuration values for the Subcluster\\* and Resource Entry\\* sections in each CE.\n+\n+\n+### Subcluster Configuration\n+\n+Each homogeneous set of worker node hardware is called a **subcluster**. For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format: `[Subcluster CHANGEME]`, where CHANGEME is the globally unique subcluster name (yes, it must be a **globally** unique name for the whole grid, not just unique to your site. Get creative.)\n+\n+| Option               | Values Accepted             | Explanation                                                                                        |\n+|----------------------|-----------------------------|----------------------------------------------------------------------------------------------------|\n+| **name**             | String                      | The same name that is in the Section label; it should be **globally unique**                       |\n+| **ram\\_mb**          | Positive Integer            | Megabytes of RAM per node                                                                          |\n+| **cores\\_per\\_node** | Positive Integer            | Number of cores per node                                                                           |\n+| **allowed\\_vos**     | Comma-separated List or `*` | The VOs that are allowed to run jobs on this subcluster (autodetected if `*`). Optional on OSG 3.3 |\n+\n+The following attributes are optional:\n+\n+| Option            | Values Accepted  | Explanation                                                                                                                |\n+|-------------------|------------------|----------------------------------------------------------------------------------------------------------------------------|\n+| max\\_wall\\_time   | Positive Integer | Maximum wall-clock time, in minutes, that a job is allowed to run on this subcluster (the default is one day or 1440 mins) |\n+| queue             | String           | The queue to which jobs should be submitted in order to run on this subcluster                                             |\n+| extra\\_transforms | Classad          | Transformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster   |\n+\n+**OSG 3.4 changes:**\n+\n+-   `allowed_vos` is mandatory\n+\n+\n+### Resource Entry Configuration (ATLAS only)\n+\n+If you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new `Resource Entry` section with a unique name in the following format: `[Resource Entry RESOURCE]` where RESOURCE is a globally unique resource name (it must be a **globally** unique name for the whole grid, not just unique to your site). The following options are required for the `Resource Entry` section and are used to generate the data required by AGIS:\n+\n+| Option                                    | Values Accepted             | Explanation                                                                                      |\n+|-------------------------------------------|-----------------------------|--------------------------------------------------------------------------------------------------|\n+| **name**                                  | String                      | The same name that is in the `Resource Entry` label; it must be **globally unique**              |\n+| **max\\_wall\\_time**                       | Positive Integer            | Maximum wall-clock time, in minutes, that a job is allowed to run on this resource               |\n+| **queue**                                 | String                      | The queue to which jobs should be submitted to run on this resource                              |\n+| **cpucount** (alias **cores\\_per\\_node**) | Positive Integer            | Number of cores that a job using this resource can get                                           |\n+| **maxmemory** (alias **ram\\_mb**)         | Positive Integer            | Maximum amount of memory (in MB) that a job using this resource can get                          |\n+| **allowed\\_vos**                          | Comma-separated List or `*` | The VOs that are allowed to run jobs on this resource (autodetected if `*`). Optional on OSG 3.3 |\n+\n+The following attributes are optional:\n+\n+| Option      | Values Accepted      | Explanation                                                                                                         |\n+|-------------|----------------------|---------------------------------------------------------------------------------------------------------------------|\n+| subclusters | Comma-separated List | The physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file |\n+| vo\\_tag     | String               | An arbitrary label that is added to jobs routed through this resource                                               |\n+\n+**OSG 3.4 changes:**\n+\n+-   `allowed_vos` is mandatory\n+\n+\n+Gateway\n+-------\n+\n+This section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses `condor-ce` as the gateway.\n+\n+This section is contained in `/etc/osg/config.d/10-gateway.ini` which is provided by the `osg-configure-gateway` RPM.\n+\n+| Option                         | Values Accepted | Explanation                                                                                                                                                                                                            |\n+|--------------------------------|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **htcondor\\_gateway\\_enabled** | `True`, `False` | (default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.                               |\n+| **job\\_envvar\\_path**          | String          | The value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.                                               |\n+\n+\n+Local Settings\n+--------------\n+\n+This section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the `osg-local-job-environment.conf` verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The `osg-local-job-environment.conf` file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.\n+\n+Adding a line such as `My_Setting = my_Value` would result in the an environment variable called `My_Setting` set to `my_Value` in the job's environment. `my_Value` can also be defined in terms of an environment variable (i.e `My_Setting = $my_Value`) that will be evaluated on the worker node. For example, to add a variable `MY_PATH` set to `/usr/local/myapp`, you'd have the following:\n+\n+``` ini\n+[Local Settings]\n+\n+MY_PATH = /usr/local/myapp\n+```\n+\n+This section is contained in `/etc/osg/config.d/40-localsettings.ini` which is provided by the `osg-configure-ce` RPM.\n+\n+\n+Misc Services\n+-------------\n+\n+This section handles the configuration of services that do not have a dedicated section for their configuration.\n+\n+This section is contained in `/etc/osg/config.d/10-misc.ini` which is provided by the `osg-configure-misc` RPM.\n+\n+This section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the [authentication section of the HTCondor-CE install documents](../compute-element/install-htcondor-ce#configuring-authentication).\n+\n+| Option                                | Values Accepted                                | Explanation                                                                                                                                                                                                                                                                                                                                                                          |\n+|---------------------------------------|------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| glexec\\_location                      | String                                         | This gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g. `$FOO`) that will be evaluated on the worker node. If it is not installed, set this to `UNAVAILABLE`. glExec does not work with the `vomsmap` authorization method on OSG 3.3 and **is entirely unsupported starting in OSG 3.4** |\n+| **gums\\_host**                        | String                                         | This setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to `xacml`. If GUMS is not used, this should be set to `UNAVAILABLE`. **GUMS is deprecated in OSG 3.4**                                                                                                                              |\n+| **authorization\\_method**             | `gridmap`, `xacml`, `local-gridmap`, `vomsmap` | This indicates which authorization method your site uses. **`xacml`** **is deprecated in OSG 3.4**                                                                                                                                                                                                                                                                                   |\n+| edit\\_lcmaps\\_db                      | `True`, `False`                                | (Optional, default True) If true, osg-configure will overwrite `/etc/lcmaps.db` to set your authorization method. The previous version will be backed up to `/etc/lcmaps.db.pre-configure`                                                                                                                                                                                           |\n+| copy\\_host\\_cert\\_for\\_service\\_certs | `True`, `False`                                | (Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS                                                                                                                                                                                                                               |\n+\n+**OSG 3.4 changes:**\n+\n+-   `glexec_location` must be `UNAVAILABLE` or unset\n+-   `authorization_method` defaults to `vomsmap`\n+-   `authorization_method` will raise a warning if set to `xacml`\n+\n+\n+Site Information\n+----------------\n+\n+The settings found in the `Site Information` section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.\n+\n+This section is contained in `/etc/osg/config.d/40-siteinfo.ini` which is provided by the `osg-configure-ce` RPM.\n+\n+| Option              | Values Accepted   | Description                                                                                                                                  |\n+|---------------------|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------|\n+| **group**           | `OSG` , `OSG-ITB` | This should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG |\n+| **host\\_name**      | String            | This should be set to be hostname of the CE that is being configured                                                                         |\n+| **resource**        | String            | The resource name of this CE endpoint as registered in OIM.                                                                                  |\n+| **resource\\_group** | String            | The resource\\_group of this CE as registered in OIM.                                                                                         |\n+| **sponsor**         | String            | This should be set to the sponsor of the resource. See note.                                                                                 |\n+| **site\\_policy**    | Url               | This should be a url pointing to the resource's usage policy                                                                                 |\n+| **contact**         | String            | This should be the name of the resource's admin contact                                                                                      |\n+| **email**           | Email address     | This should be the email address of the admin contact for the resource                                                                       |\n+| **city**            | String            | This should be the city that the resource is located in                                                                                      |\n+| **country**         | String            | This should be two letter country code for the country that the resource is located in.                                                      |\n+| **longitude**       | Number            | This should be the longitude of the resource. It should be a number between -180 and 180.                                                    |\n+| **latitude**        | Number            | This should be the latitude of the resource. It should be a number between -90 and 90.                                                       |\n+\n+!!! note\n+    **sponsor**:<br/>\n+    If your resource has multiple sponsors, you can separate them using commas\n+    or specify the percentage using the following format 'osg, atlas, cms' or\n+    'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\n+    sponsors are used. If you have a sponsor that is not an OSG VO, you can\n+    indicate this by using 'local' as the VO.\n+\n+\n+Squid\n+-----\n+\n+This section handles the configuration and setup of the squid web caching and proxy service.\n+\n+This section is contained in `/etc/osg/config.d/01-squid.ini` which is provided by the `osg-configure-squid` RPM.\n+\n+| Option      | Values Accepted           | Explanation                                                    |\n+|-------------|---------------------------|----------------------------------------------------------------|\n+| **enabled** | `True`, `False`, `Ignore` | This indicates whether the squid service is being used or not. |\n+| location    | String                    | This should be set to the `hostname:port` of the squid server. |\n+\n+\n+Storage\n+-------\n+\n+This section gives information about the options in the Storage section of the configuration file.\n+Several of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\n+Please review the Storage Related Parameters section of the [Environment Variables](EnvironmentVariables) description\n+as well as the [Overview of Services](OverviewOfServicesInOSG) and [Site Planning](SitePlanning) discussions", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/40#discussion_r138411170", 
  "id": 138411170, 
  "original_commit_id": "2633a2cdeec320b54ac9fa26675eabd19ac44ac0", 
  "original_position": 505, 
  "path": "docs/other/configuration-with-osg-configure.md", 
  "position": null, 
  "pull_request_review_id": 62201828, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/40", 
  "updated_at": "2017-09-12T18:55:50Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/138411170", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
