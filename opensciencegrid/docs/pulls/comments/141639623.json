{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/181#discussion_r141639623"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/181"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/141639623"
    }
  }, 
  "author_association": "OWNER", 
  "body": "Why reproduce the entirety of https://opensciencegrid.github.io/docs/common/ca/#install-fetch-crl when you can just link it?", 
  "commit_id": "8f4c7863db5102d1a58db004ce44134d73c4fe27", 
  "created_at": "2017-09-28T14:46:45Z", 
  "diff_hunk": "@@ -0,0 +1,1499 @@\n+**Hadoop 2.0.0 (CDH4)**\n+=======================\n+\n+<span class=\"twiki-macro DOC_STATUS_TABLE\"></span> \n+**Purpose**: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.\n+\n+!!! warning\n+    If you are installing Hadoop/Bestman from OSG 3.1, you will need to use [this](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallHadoopSE) guide instead. This guide details installing Hadoop 2.0 from the OSG 3.2 repositories.\n+\n+Preparation\n+===========\n+\n+Introduction\n+------------\n+\n+[Hadoop Distributed File System](http://hadoop.apache.org/hdfs/) (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:\n+\n+-   An [SRM interface](https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.html) for grid access;\n+-   GridFTP-HDFS as transport layer; and\n+-   A [FUSE interface](http://fuse.sourceforge.net/) for localized POSIX access.\n+-   [Apache Hadoop](http://hadoop.apache.org/)\n+\n+The OSG packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs and are available from the OSG repositories. It is also recommended that you enable [EPEL](http://fedoraproject.org/wiki/EPEL) repos.\n+\n+Note on upgrading from Hadoop 0.20\n+----------------------------------\n+\n+!!! note\n+    If upgrading, make sure to follow these instructions %RED%BEFORE%ENDCOLOR% any other instructions in this document.\n+\n+1. First, you must upgrade to the newest version of Hadoop-0.20. Older versions may have dependency and upgrade problems. Make sure that your version is at least `hadoop-0.20-0.20.2+737-26` (or newer) on all nodes. (The important number is the 26. Older release numbers may have upgrade problems). You may need to specify this version specifically to ensure that the correct version is installed. ie. `yum upgrade hadoop-0.20-0.20.2+737-26`.\n+1. Next, make sure all configuration and important files are backed up in case of catastrophic failure. In particular, backup a copy of `hdfs-site.xml`, `core-site.xml` and important namenode files.\n+1. Now, upgrade to hadoop-2.0.0 using `yum upgrade hadoop`\n+1. Also, make sure to bring in any new packages using the relevant meta-package, such as `yum install osg-se-hadoop-namenode`, `yum install osg-se-hadoop-datanode` or `yum install osg-se-hadoop-srm`.\n+1. On the namenode, run `hadoop namenode -upgrade` to upgrade the meta-data catalog.\n+1. Follow the configuration instructions below for each node. In particular, restore or modify `hdfs-site.xml` and `core-site.xml` then copy to all nodes. For any nodes using fuse mounts, note that \"hdfs\\#\" should be changed to \"hadoop-fuse-dfs\\#\" in `/etc/fstab`.\n+\n+Requirements\n+============\n+\n+Architecture\n+------------\n+\n+!!! note\n+    There are several important components to a storage element installation. Throughout this document, it will be stated which node the relevant installation instructions apply to. It can apply to one of the following:\n+\n+-   **Namenode**: You will have at least one namenode. The name node functions as the directory server and coordinator of the hadoop cluster. It houses all the meta-data for the hadoop cluster. %RED%The namenode and secondary namenode need to have a directory that they can both access on a shared filesystem so that they can exchange filesystem checkpoints.%ENDCOLOR%\n+-   **Secondary Namenode**: This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage. This dramatically improves startup and restart times.\n+-   **Datanode**: You will have many datanodes. Each data node stores large blocks of files to be stored on the hadoop cluster.\n+-   **Client**: This is a documentation shorthand that refers to any machine with the hadoop client commands and [FUSE](http://fuse.sourceforge.net/) mount. Any machine that needs a FUSE mount to access data in a POSIX-like fashion will need this.\n+-   **GridFTP node**: This is a node with [Globus GridFTP](http://dev.globus.org/wiki/GridFTP). The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration. You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.\n+-    **SRM node**: This node will contain the BeStMan SRM frontend for accessing the Hadoop cluster via the SRM protocol. [BeStMan2 SRM](https://sdm.lbl.gov/bestman/)\n+\n+Note that these components are not necessarily mutually exclusive. For instance, you may consider having your GridFTP server co-located on the SRM node. Alternatively, you can locate a client (or even a GridFTP node) co-located on each data node. That way, each data node also acts as an access point to the hadoop cluster.\n+\n+Please read the [planning document](https://twiki.opensciencegrid.org/bin/view/Documentation/HadoopUnderstanding) to understand different components of the system.\n+\n+!!! note\n+    Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email <osg-storage@opensciencegrid.org> and <osg-hadoop@opensciencegrid.org>.\n+\n+Host and OS\n+-----------\n+\n+Hadoop will run anywhere that Java is supported (including Solaris). However, these instructions are for RedHat derivants (including Scientific Linux) because of the RPM based installation. The current supported Operating Systems supported by the OSG are Red Hat Enterprise Linux 6, 7, and variants (see [details...](../release/supported_platforms)).\n+\n+\n+The HDFS prerequisites are:\n+\n+-   Minimum of 1 headnode (the namenode)\n+-   At least one node which will hold data, preferably at least 2. Most sites will have 20 to 200 datanodes.\n+-   Working Yum and RPM installation on every system.\n+-   `fuse` kernel module and `fuse-libs`.\n+-   Java RPM. If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency. Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.\n+\n+**Compatibility Note** Note that versions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.\n+\n+Users\n+-----\n+\n+This installation will create following users unless they are already created.\n+\n+| User      | Comment                                           |\n+|:----------|:--------------------------------------------------|\n+| `bestman` | Used by Bestman SRM server (needs sudo access).   |\n+| `hdfs`    | Used by Hadoop to store data blocks and meta-data |\n+\n+For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n+\n+For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n+\n+For gums users, this means that each user that can be authenticated by gums should be created on the server.\n+\n+Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n+\n+Certificates\n+------------\n+\n+<span class=\"twiki-macro STARTSECTION\">Certificates</span>\n+\n+| Certificate                 | User that owns certificate | Path to certificate                                                                                 |\n+|:----------------------------|:---------------------------|:----------------------------------------------------------------------------------------------------|\n+| Host certificate            | `root`                     | `/etc/grid-security/hostcert.pem` <br> `/etc/grid-security/hostkey.pem`                       |\n+| Bestman service certificate | `bestman`                  | `/etc/grid-security/bestman/bestmancert.pem` <br> `/etc/grid-security/bestman/bestmankey.pem` |\n+\n+<span class=\"twiki-macro ENDSECTION\">Certificates</span>\n+\n+[Instructions](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/InstallCertScripts) to request a service certificate.\n+\n+You will also need a copy of CA certificates (see below). Note that the `osg-se-hadoop-srm` and `osg-se-hadoop-gridftp` package will automatically install a certificate package but will not necessarily pick the cert package you expect. For instance, certain installs will prefer the `osg-ca-scripts` package to fulfill this requirement, which installs a set of scripts to automatically update the certificates, but does not initialize the CA certs by default (you have to run it first). For this reason, you may want to specifically install the cert package of your choice first, before installing Hadoop.\n+\n+Networking\n+----------\n+\n+For more details on overall Firewall configuration, please see our [Firewall documentation](https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/FirewallInformation).\n+\n+**NOTE:** The versions of Hadoop in OSG series 3.1 and 3.2 (ie, Hadoop 0.20 and 2.0.0) do not inter-operate. In order to use Hadoop 2.0.0, *all* nodes in the hadoop system (namenode, secondary namenode, datanodes, srm/gridftp nodes and all client nodes) must update to OSG 3.2 and Hadoop 2.0.0.\n+\n+Initializing Certificate Authority\n+==================================\n+\n+This is needed by GridFTP and SRM nodes, but it is recommended for all nodes in the cluster.\n+\n+++ Enable and Start `fetch-crl` To enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/181#discussion_r141639623", 
  "id": 141639623, 
  "original_commit_id": "f99ad729a9c507e9ede7001636d8e09ca20079aa", 
  "original_position": 123, 
  "path": "docs/data/install-hadoop-2-0-0.md", 
  "position": null, 
  "pull_request_review_id": 65881447, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/181", 
  "updated_at": "2017-10-13T17:11:24Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/141639623", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
