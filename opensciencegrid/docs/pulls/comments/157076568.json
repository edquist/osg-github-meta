{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/docs/pull/271#discussion_r157076568"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/271"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/157076568"
    }
  }, 
  "author_association": "OWNER", 
  "body": "The first sentence is weird, get rid of it", 
  "commit_id": "4c9f6a729fc81dfb4f7176cfce003a5500864419", 
  "created_at": "2017-12-14T22:13:03Z", 
  "diff_hunk": "@@ -0,0 +1,375 @@\n+Troubleshooting Gratia Accounting\n+=================================\n+\n+This document will help you troubleshoot problems with the Gratia Accounting, particularly with problems in collecting and reporting accounting information to the central OSG accounting service. See also other documents recommended in the Reference section below.\n+\n+\n+Gratia/GRACC: The Big Picture\n+-----------------------\n+\n+Gratia is software used in OSG to gather accounting information. The information is collected from individual resources at a site, such as a Compute Element or a a submission host. The program that collects the data is called a \"Gratia probe\". The information is transferred to a GRACC server. Most sites will choose to send the accounting data to the central OSG Gratia server, but you can also use a Gratia server at your site (which can send forward the data to the central OSG Gratia server). Here is a diagram:\n+\n+!!! note \"Difference between Gratia and GRACC\"\n+    Gratia is the legacy name of the OSG Accounting system.  GRACC is the new name of the server and hosted components of the accounting system.  When we refer to Gratia, we mean either the data or the probes on the resources.  If we mention GRACC, we are referring to the hosted components that the OSG maintains.\n+\n+![Gratia Basics](../images/gratia-overview.png)\n+\n+These are the definitions of the major elements in the above figure.\n+\n+-   **Gratia probe**: A piece of software that collects accounting data from the computer on which it's running, and transmits it to a Gratia server.\n+-   **GRACC server**: A server that collects Gratia accounting data from one or more sites and can share it with users via a web page.  The GRACC server is hosted by the OSG.\n+-   **Reporter**: A web service running on the GRACC server. Users can connect to the reporter via a web browser to explore the Gratia data.\n+-   **Collector**: A web service running on the GRACC server that collects data from one or more Gratia probes. Users do not directly interact with the collector.\n+\n+You can see the OSG's GRACC website at https://gracc.opensciencegrid.org.\n+\n+You can see a fancier version of the Gratia data at [display.grid.iu.edu](http://display.grid.iu.edu/). This is **not** running a Gratia collector, but is a separate service.\n+\n+Gratia Probes\n+-------------\n+\n+Gratia Probes are periodically run as cron jobs, but different probes will run at different intervals. The cron jobs will always run and you should not remove them. You can find them in `/etc/cron.d`.\n+\n+However, the cron jobs will only do anything if you have enabled them. You enable them via an init script. For example, to enable them:\n+\n+    :::console\n+    root@host # service gratia-probes-cron start\n+    Enabling gratia probes cron:                               [  %GREEN%OK%ENDCOLOR%  ]\n+\n+To disable them:\n+\n+    :::console\n+    root@host # service gratia-probes-cron stop\n+    Disabling gratia probes cron:                               [  %GREEN%OK%ENDCOLOR%  ]\n+\n+You also need to enable individual probes, usually via `osg-configure`.  Documentation on using `osg-configure` with Gratia [documented elsewhere](configuration-with-osg-configure#gratia).\n+\n+### Running Gratia Probes\n+\n+When the cron jobs are enabled and run, they go through the following process, with minor changes between different Gratia probes:\n+\n+1.  The probe is invoked. It reads its configuration from `/etc/gratia/%RED%PROBE-NAME%ENDCOLOR%/ProbeConfig`.\n+2.  It collects the accounting information from the underlying system. For example, the Condor probe will read it from the `PER_JOB_HISTORY_DIR`, which is usually `/var/lib/gratia/data`.\n+3.  It transforms the data into Gratia records and saves them into `/var/lib/gratia/tmp/gratiafiles/`\n+4.  When there are sufficient Gratia records, or when sufficient time has passed, it uploads sets of records in batches to the GRACC server, then removes them from the `gratiafiles` directory.\n+5.  All progress is logged to `/var/log/gratia`.\n+6.  If there are failures in uploading the files to the GRACC server\n+    1.  Files are not removed from `gratiafiles` until they are successfully uploaded.\n+    2.  Errors are logged to log files in `/var/log/gratia`.\n+    3.  The uploads will be tried again later.\n+\n+### Gratia Probe Configuration\n+\n+In normal cases, `osg-configure` does the editing of the probe configuration files, at least on the CE. The configuration is found in `/etc/osg/config.d/30-gratia.ini` and [documented elsewhere](configuration-with-osg-configure#gratia).\n+\n+If there are problems or special configuration, you might need to edit the Gratia configuration files yourself. Each probe has a separate configuration file found in `/etc/gratia/%RED%PROBE-NAME%ENDCOLOR%/ProbeConfig`.\n+\n+The ProbeConfig files have many details. A few options that you might need to edit are shown before. This is **not** a complete file, but only shows a subset of the options.\n+\n+``` file\n+<ProbeConfiguration \n+\n+    CollectorHost=\"gratia-osg-itb.opensciencegrid.org:80\"\n+    SSLHost=\"gratia-osg-itb.opensciencegrid.org:80\"\n+    SSLRegistrationHost=\"gratia-osg-itb.opensciencegrid.org:80\"\n+\n+    ProbeName=\"condor:fermicloud084.fnal.gov\"\n+    SiteName=\"WISC_OSG_EDU\"\n+    EnableProbe=\"1\"\n+/>\n+```\n+\n+The options you see here are:\n+\n+| Option              | Comments                                                                               |\n+|:--------------------|:---------------------------------------------------------------------------------------|\n+| CollectorHost       | The GRACC server this probe reports to                                                |\n+| SSLHost             | The GRACC server this probe reports to                                                |\n+| SSLRegistrationHost | The GRACC server this probe reports to                                                |\n+| ProbeName           | The unique name for this probe. Note that it includes the probe type and the host name |\n+| SiteName            | The name of your site, as registered in OIM. If your site must be registered in OIM    |\n+| EnableProbe         | The probe will only run if this is \"1\"                                                 |\n+\n+Again, there are many more options in this file. Most of the time you won't need to touch them.\n+\n+Are the Gratia cron jobs running? \n+---------------------------------\n+\n+You should make sure the Gratia cron jobs are running. The simplest way is with the `service` command:\n+\n+    :::console\n+    root@host # /sbin/service gratia-probes-cron status\n+    gratia probes cron is enabled.\n+\n+If it is not enabled, enable it as described above.\n+\n+A future release of Gratia will provide status on each of the individual probes, but right now this only ensures that the basic cron job is running. In the meantime, you can check if the individual Gratia probes are enabled. To do this, look at the `EnableProbe` option in the `ProbeConfig` file, as described above. A quick command to do this is shown here. Note that the Condor and GridFTP Transfer probes are enabled while the glexec probe is disabled:\n+\n+    :::console\n+    root@host # cd /etc/gratia\n+    root@host # grep -r EnableProbe *\n+    condor/ProbeConfig:    EnableProbe=\"1\"\n+    glexec/ProbeConfig:    EnableProbe=\"0\"\n+    gridftp-transfer/ProbeConfig:    EnableProbe=\"1\"\n+\n+If you see no log files in `/var/log/gratia` you may have an error in the probe configuration file. Run manually the test for your probe (check `/etc/cron.d/gratia-probe-condor.cron`), e.g. `/usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig`. If there is an error you may get a suggestion on where it is, e.g.:\n+\n+    :::console\n+    root@host # /usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig\n+    Parse error in /etc/gratia/condor/ProbeConfig: not well-formed (invalid token): line 21, column 4\n+\n+Correct the error and restart gratia.\n+\n+Have you configured the resource names correctly? \n+-------------------------------------------------\n+\n+Do the names of your resources match the names in OIM?  Gratia retrieves the resource name from the `Site Information` section of the `/etc/osg/config.d/40-siteinfo.ini`\n+\n+\n+``` file\n+;===================================================================\n+;                       Site Information\n+;===================================================================\n+\n+[Site Information]\n+; The group option indicates the group that the OSG site should be listed in,\n+; for production sites this should be OSG, for vtb or itb testing it should be\n+; OSG-ITB\n+; \n+; YOU WILL NEED TO CHANGE THIS\n+group = OSG\n+\n+; The host_name setting should give the host name of the CE  that is being \n+; configured, this setting must be a valid dns name that resolves\n+; \n+; YOU WILL NEED TO CHANGE THIS\n+host_name = tusker-gw1.unl.edu\n+\n+; The resource setting should be set to the same value as used in the OIM \n+; registration at the goc \n+; \n+; YOU WILL NEED TO CHANGE THIS\n+resource = Tusker-CE1\n+\n+; The resource_group setting should be set to the same value as used in the OIM \n+; registration at the goc \n+; \n+; YOU WILL NEED TO CHANGE THIS\n+resource_group = Tusker\n+\n+```\n+\n+Do those names match the names that you registered with OIM? If not, edit the names, and rerun \"osg-configure -c\".\n+\n+Did the site name change?\n+-------------------------\n+\n+Was the site previously reporting data, but the site name (not host name, but site name) changed? When the site name changes, you need to ask the Gratia operations team to update the name of your site at the GRACC collector. To do this:\n+\n+1.  Open a ticket at [the GOC ticket web page](https://ticket.grid.iu.edu/goc/submit)\n+2.  Select \"Software or Service\"\n+3.  Select \"GRACC Operations\"\n+4.  Type a friendly email that asks the GRACC team to change your site name at the collector. Make sure to tell them the old name and the new name.\n+\n+ Is a site reporting data?\n+--------------------------\n+\n+You can see if the OSG GRACC Server is getting data from a site by going to [GRACC](https://gracc.opensciencegrid.org/dashboard/db/pilot-jobs-summary?orgId=1):\n+\n+1.  Specify the site name in Facility\n+\n+Condor must be configured to put information about each job into a special directory.\n+-------------------------------------------------------------------------------------\n+\n+Gratia will read and remove the files in order to collect the accounting information.\n+\n+The configuration variable is called `PER_JOB_HISTORY_DIR`. If you install the OSG RPM for Condor, the Gratia probe will extend its configuration by adding a file to `/etc/condor/config.d`, and will set this variable to `/var/lib/gratia/data`. If you are using a different installation method, you probably need to set the variable yourself. You can check if it's set by using `condor_config_val`, like this:\n+\n+    :::console\n+    user@host $ condor_config_val -v PER_JOB_HISTORY_DIR\n+    PER_JOB_HISTORY_DIR: /var/lib/gratia/data\n+        Defined in '/etc/condor/config.d/99_gratia.conf', line 5.\n+\n+If you set this value, you need to restart condor:\n+\n+    :::console\n+    root@host # condor_restart\n+    Sent \"Restart\" command to local master\n+\n+Unlike many Condor settings, a **condor\\_reconfig** is not sufficient - you must restart!\n+\n+If you accidentally did not set `PER_JOB_HISTORY_DIR` (see above)\n+-----------------------------------------------------------------\n+\n+The HTCondor Gratia probe will not publish accounting information about jobs without `PER_JOB_HISTORY_DIR`. You can have Gratia read the Condor history file and publish data that way. If you know the time period of the missing data, you should specify a start and end times. This reduces the load on the Gratia collector. To do so:\n+\n+    :::console\n+    %BLUE%Preferred method using start and end times%ENDCOLOR%\n+    root@host # /usr/share/gratia/condor/condor_meter --history --start-time=\"2014-06-01\" --end-time=\"2014-06-02\" --verbose\n+    2014-06-03 10:00:36 CDT Gratia: RUNNING condor_meter MANUALLY using HTCondor history from 2014-06-01 to 2014-06-02\n+    2014-06-03 10:00:36 CDT Gratia: RUNNING: condor_history -l -constraint '((JobCurrentStartDate > 1401598800) && (JobCurrentStartDate < 1401685200))'\n+    2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records submitted: 399\n+    2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records found: 400\n+    2014-06-03 10:00:49 CDT Gratia: RUNNING condor_meter MANUALLY Finished\n+\n+    %BLUE% or if you need to go back to the beginning of time%ENDCOLOR%\n+    root@host # /usr/share/gratia/condor/condor_meter --history --verbose\n+    2014-06-03 10:06:19 CDT Gratia: RUNNING condor_meter MANUALLY using all HTCondor history\n+    2014-06-03 10:06:19 CDT Gratia: RUNNING: condor_history -l\n+    2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records submitted: 13026\n+    2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records found: 13027\n+    2014-06-03 10:11:38 CDT Gratia: RUNNING condor_meter MANUALLY Finished\n+\n+\n+Not much is printed to the screen, but you can see progress in the Gratia log file:\n+\n+    13:35:28 CDT Gratia: Initializing Gratia with /etc/gratia/condor/ProbeConfig\n+    13:35:28 CDT Gratia: Creating a ProbeDetails record 2012-04-04T18:35:28Z\n+    13:35:28 CDT Gratia: ***********************************************************\n+    13:35:28 CDT Gratia: OK - Handshake added to bundle (1/100)\n+    13:35:28 CDT Gratia: ***********************************************************\n+    13:35:28 CDT Gratia: List of backup directories: [u'/var/lib/gratia/tmp']\n+    13:35:28 CDT Gratia: Reprocessing response: OK - Reprocessing 0 record(s) uploaded, 0 bundled, 0 failed\n+    13:35:28 CDT Gratia: After reprocessing: 0 in outbox 0 in staged outbox 0 tar files\n+    13:35:28 CDT Gratia: Creating a UsageRecord 2012-04-04T18:35:28Z\n+    ...\n+    13:35:29 CDT Gratia: Processing bundle file: \n+    13:35:29 CDT Gratia: Processing bundle file: /var/lib/gratia/tmp/gratiafiles/\n+        subdir.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80/\n+        outbox/r.18425.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80.gratia.xml__BSuXo18428\n+    ...\n+    13:35:29 CDT Gratia: ***********************************************************\n+    13:35:29 CDT Gratia: Removing log files older than 31 days from /var/log/gratia\n+    13:35:29 CDT Gratia: /var/log/gratia uses 0.035% and there is 73% free\n+    13:35:29 CDT Gratia: Removing incomplete data files older than 31 days from /var/lib/gratia/data/\n+    13:35:29 CDT Gratia: /var/lib/gratia/data uses 0% and there is 73% free\n+    13:35:29 CDT Gratia: End of execution summary: new records sent successfully: 37\n+\n+\n+\n+!!! note \n+    Condor rotates history files, so you can only report what Condor has kept. Controlling the Condor history is documented in the Condor manual. In particular, see the options for [MAX\\_HISTORY\\_LOG](http://research.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html#param:MaxHistoryLog) and [MAX\\_HISTORY\\_ROTATIONS](http://research.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html#param:MaxHistoryRotations).\n+\n+Gratia log files: bad Gratia hostname\n+-------------------------------------\n+\n+This is an example problem where the configuration was bad: there was an incorrect hostname for the Gratia server. The problem is clearly visible in the Gratia log file, which is located in `cd /var/log/gratia/`. There is one log file per day, labeled by the date:\n+\n+    :::console\n+    root@host # cd /var/log/gratia/\n+    root@host # cat 2012-04-03.log \n+    ...\n+    %RED%You can see that Gratia is using the correct configuration file:%ENDCOLOR%\n+    15:06:55 CDT Gratia: Using config file: /etc/gratia/condor/ProbeConfig\n+\n+    %RED%Here Gratia is removing a file from the Condor PER_JOB_HISTORY_DIR and creating a Gratia accounting record for it%ENDCOLOR%\n+    15:06:55 CDT Gratia: Creating a UsageRecord 2012-04-03T20:06:55Z\n+    15:06:55 CDT Gratia: Registering transient input file: /var/lib/gratia/data/history.37.0\n+    15:06:55 CDT Gratia: ***********************************************************\n+    15:06:55 CDT Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/\n+        subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/\n+        outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606\n+    15:06:55 CDT Gratia: Deleting transient input file: /var/lib/gratia/data/history.37.0\n+\n+    %RED%Later, Gratia failed to connect to the server due to a bad hostname%ENDCOLOR%\n+    15:06:55 CDT Gratia: Failed to send xml to web service due to an error of type \"socket.gaierror\": (-2, 'Name or service not known')\n+    ...\n+    15:06:55 CDT Gratia: Response indicates failure, the following files will not be deleted:\n+    15:06:55 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/\n+        subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/\n+        outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606\n+\n+\n+If you accidentally had a bad Gratia hostname, you probably want to recover your Gratia data. \n+---------------------------------------------------------------------------------------------------\n+\n+This can be done, though it's not simple. There are a few things you need to do. But first, you need to understand exactly where Gratia stores files.\n+\n+When a Gratia extracts accounting information, it creates one file per record and stores it in a directory. The directory is a long name that contains the type of the probe (such as `condor`), the name of the host you're running on, and the name of the GRACC host you're sending the information to. For simplicity, lets call that name *probe-records*, but you'll see what it really looks like below. Within this directory, you'll see some subdirectories:\n+\n+|  Directory                                                                | Purpose                                       |\n+|:--------------------------------------------------------------------------|:----------------------------------------------|\n+| /var/lib/gratia/tmp/grataifiles/%RED%probe-records%ENDCOLOR%/outbox       | The usual location for the accounting records |\n+| /var/lib/gratia/tmp/grataifiles/%RED%probe-records%ENDCOLOR%/staged/store | An overflow location when there are problems  |\n+\n+When you recover old records, you need to:\n+\n+1.  Move files from the outbox of the incorrect *probe-records* directory into the outbox of the correctly named *probe-records* directory.\n+2.  Move tarred and compressed files from the staged/store of the incorrect *probe-records* directory into the staged/store of the correctly named *probe-records* directory. Then you uncompress them and remove the compressed version.\n+\n+In the examples below, the hostname for gratia was \"accidentally\" spelled backwards. Instead of `gratia-osg-itb.opensciencegrid.org`, it was `aitarg-osg-itb.opensciencegrid.org`.\n+\n+1. First you need to fix the hostname. For a CE, you can edit `/etc/osg/config.d/30-gratia.ini` and rerun `osg-configure -c`. In other installations, you have to edit the appropriate `ProbeConfig` file.\n+\n+2. Next, submit a job via to your batch system, then run the appropriate Gratia probe (or wait for it to run via cron). This will create the properly named directories on your disk. For example:\n+\n+    As a user: \n+\n+        :::console\n+        user@host $ globus-job-run fermicloud084.fnal.gov/jobmanager-condor /bin/hostname \n+\n+    As root (adjust for your batch system): \n+\n+        :::console\n+        root@host # /share/gratia/condor/condor\\_meter\n+\n+3. Find the Gratia records that can be easily uploaded. They are located in a a directory with an unwieldly name that includes your hostname and the incorrect name of the Gratia host. You can see the directory name in the Gratia log: the misspelled name is noted in red below, but *it will be different on your computer*.\n+\n+        :::console\n+        user@host $ less /var/log/gratia/2012-04-06\n+        ...\n+        16:04:29 CDT Gratia: Response indicates failure, the following files will not be deleted:\n+        16:04:29 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/\n+            subdir.condor_fermicloud084.fnal.gov_%RED%aitarg%ENDCOLOR%-osg-itb.opensciencegrid.org_80/\n+            outbox/r.916.condor_fermicloud084.fnal.gov_aitarg-osg-itb.opensciencegrid.org_80.gratia.xml__JDlHbNb918\n+\n+    (The filename was wrapped for legibility.)\n+\n+    You can simply copy these to the correct directory. Wait for the Gratia cron job to run, or force it to run.\n+\n+        :::console\n+        root@host # cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%aitarg%ENDCOLOR%-osg-itb.opensciencegrid.org_80/outbox/.\n+        root@host # mv * /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%gratia%ENDCOLOR%-osg-itb.opensciencegrid.org_80/outbox/.\n+\n+\n+4. If this has been a persistent problem, you might have many records. After a while, they are put into a compressed files in another directory. You can move those files, then uncompress them. This is a long name: note that the path ends in \"staged/store\" instead of \"outbox\" as above:\n+\n+        :::console\n+        %RED%# Find the old files%ENDCOLOR%\n+        root@host # cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%aitarg%ENDCOLOR%-osg-itb.opensciencegrid.org_80/staged/store\n+\n+        %RED%# Move them to the correct directory%ENDCOLOR%\n+        root@host # mv tz* /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%gratia%ENDCOLOR%-osg-itb.opensciencegrid.org_80/outbox/.\n+        root@host # cd !$\n+\n+        %RED%# For each tz file:%ENDCOLOR%\n+        root@host # tar xf tz.1223.... [name shortened for legibility]\n+        root@host # rm tz.1223....\n+\n+\n+    When you've done this, you can re-run the Gratia probe by hand, or wait for it to run via cron.\n+\n+Appendix: Important Gratia files\n+--------------------------------\n+\n+This document cannot cover all the errors you might experience. If you need to look for more data, you can look at log files for the various services on your CE.", 
  "html_url": "https://github.com/opensciencegrid/docs/pull/271#discussion_r157076568", 
  "id": 157076568, 
  "original_commit_id": "ebc3f346824e46557b3a09f15ad4cccf83dc59c5", 
  "original_position": 355, 
  "path": "docs/other/troubleshooting-gratia.md", 
  "position": null, 
  "pull_request_review_id": 83653096, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/docs/pulls/271", 
  "updated_at": "2018-01-10T21:33:15Z", 
  "url": "https://api.github.com/repos/opensciencegrid/docs/pulls/comments/157076568", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
