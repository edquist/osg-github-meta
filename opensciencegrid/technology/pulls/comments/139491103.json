{
  "_links": {
    "html": {
      "href": "https://github.com/opensciencegrid/technology/pull/307#discussion_r139491103"
    }, 
    "pull_request": {
      "href": "https://api.github.com/repos/opensciencegrid/technology/pulls/307"
    }, 
    "self": {
      "href": "https://api.github.com/repos/opensciencegrid/technology/pulls/comments/139491103"
    }
  }, 
  "author_association": "OWNER", 
  "body": "Get rid of this section and find a different way to bug @matyasselmeci about cleaning up the \"old stuff\"", 
  "commit_id": "6d11ad52d0380adcfd8201aa71ee5101f824ae6d", 
  "created_at": "2017-09-18T17:42:43Z", 
  "diff_hunk": "@@ -0,0 +1,304 @@\n+<style type=\"text/css\">\n+pre em { color: red; font-weight: normal; font-style: normal; }\n+.old { color: red; }\n+.off { color: blue; }\n+</style>\n+\n+# Software/Release Team ITB Site Design\n+\n+\n+## Madison ITB Machines\n+\n+All physical hosts are located in 3370A in the VDT rack.\n+\n+| Host                                         | Purpose                 | OS     | Arch       | CPU Model            | CPUs    | RAM   | Storage              | Notes                       |\n+|:---------------------------------------------|:------------------------|:-------|:-----------|:---------------------|:--------|:------|:---------------------|:----------------------------|\n+| itb-data1                                    | worker node             | SL 6.9 | x86 64-bit | Celeron G530 2.4Ghz  | 2 / 2   | 8 GB  | 750 GB \u00d7 2 (RAID?)   | planned as HDFS data node   |\n+| itb-data2                                    | worker node             | SL 6.9 | x86 64-bit | Celeron G530 2.4Ghz  | 2 / 2   | 8 GB  | 750 GB \u00d7 2 (RAID?)   | planned as HDFS data node   |\n+| itb-data3                                    | worker node             | SL 7.3 | x86 64-bit | Celeron G530 2.4Ghz  | 2 / 2   | 8 GB  | 750 GB \u00d7 2 (RAID?)   | planned as HDFS data node   |\n+| itb-data4                                    | worker node             | SL 6.9 | x86 64-bit | Celeron G530 2.4Ghz  | 2 / 2   | 8 GB  | 750 GB \u00d7 2 (RAID?)   | planned as XRootD data node |\n+| itb-data5                                    | worker node             | SL 6.9 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4   | 8 GB  | 750 GB \u00d7 2 (RAID?)   | planned as XRootD data node |\n+| itb-data6                                    | worker node             | SL 7.3 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4   | 8 GB  | ???                  | planned as XRootD data node |\n+| itb-host-1                                   | KVM host                | SL 7.3 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 1 TB \u00d7 4 (HW RAID 5) |                             |\n+| \u00a0\u00b7\u00a0 itb-ce1                                  | HTCondor-CE             | SL 6.9 | x86 64-bit | VM                   | 4       | 6 GB  | 192 GB               |                             |\n+| \u00a0\u00b7\u00a0 itb-ce2                                  | HTCondor-CE             | SL 6.9 | x86 64-bit | VM                   | 4       | 6 GB  | 192 GB               |                             |\n+| \u00a0\u00b7\u00a0 itb-cm                                   | HTCondor CM             | SL 7.3 | x86 64-bit | VM                   | 4       | 6 GB  | 192 GB               |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"old\">itb-glidein</span>     | GlideinWMS VO frontend? | SL 6.3 | x86 64-bit | VM                   | 3       | 6 GB  | 50 GB                |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"old\">itb-gums-rsv</span>    | GUMS, RSV               | SL 6.3 | x86 64-bit | VM                   | 3       | 6 GB  | 50 GB                |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"off\">itb-hdfs-name1</span>  | \u2014 (so far)              | SL ?   | x86 64-bit | VM                   | 4       | 6 GB  | 192 GB               |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"old\">itb-hdfs-name2</span>  | \u2014 (so far)              | SL 6.3 | x86 64-bit | VM                   | 3       | 6 GB  | 50 GB                |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"old\">itb-se-hdfs</span>     | \u2014 (so far)              | SL 6.3 | x86 64-bit | VM                   | 3       | 6 GB  | 50 GB                |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"old\">itb-se-xrootd</span>   | \u2014 (so far)              | SL 6.3 | x86 64-bit | VM                   | 3       | 6 GB  | 50 GB                |                             |\n+| \u00a0\u00b7\u00a0 itb-submit                               | HTCondor submit         | SL 6.9 | x86 64-bit | VM                   | 4       | 6 GB  | 192 GB               |                             |\n+| \u00a0\u00b7\u00a0 <span class=\"off\">itb-xrootd</span>      | \u2014 (so far)              | SL ?   | x86 64-bit | VM                   | 4       | 6 GB  | 192 GB               |                             |\n+| itb-host-2                                   | worker node             | SL 6.9 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 352 GB in $(EXECUTE) |                             |\n+| itb-host-3                                   | worker node             | SL 7.3 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 352 GB in $(EXECUTE) |                             |\n+\n+(Data last updated 2017-09-11 by Tim\u00a0C. <span class=\"old\">Red</span> indicates a host that has yet to be rebuilt; <span class=\"off\">Blue</span> is rebuilt but currently off.)\n+\n+\n+## ITB Goals, Revisited 2016-11-03\n+\n+Roughly in order of priority, at least for the top few items.\n+\n+- Test pre-release builds of HTCondor and HTCondor-CE in an OSG context\n+    - Install software and run jobs as soon as possible after a pre-release\n+    - An implementation idea: Maybe have more than one CE and/or HTCondor instance? One for \u201cproduction\u201d and one for\n+      rapid testing\n+- Add the ability to create and control job pressure locally\n+    - Run all the time? Some of the time? Most of the time, but be able to drain or spike on demand?\n+    - Maybe add a VO front-end?\n+- Review and significantly tighten firewall rules on ITB hosts\n+- Add HTCondor-CE-Bosco to the overall site plan, so that we can test it, too\n+- Test the OSG stack before release (i.e., updates out of osg-prerelease)\n+- Have a mix of EL6 and EL7 execute hosts and user jobs to test EL\u00a06\u21927 migration\n+- Test other batch systems\n+    - Slurm\n+    - PBS, in some flavor\n+    - LSF and SGE usage is waning in the field, so they are lowest priority\n+- Add other types of hosts (e.g., VO frontend, storage, RSV, squid, etc.)\n+    - Current HDFS/XRootD nodes could be repurposed or we could spin up new VMs on `itb-host-1`\n+    - We could potentially run our own factory in the future\n+- Investigate whether there is a way to switch easily between ITB and production pilots and payloads\n+- Test large-customer specific workflows (e.g., LIGO)\n+- Investigate monitoring/testing setup for site verification\n+    - Ask sites (e.g., UNL) for their solutions\n+- Flexible site installation\n+    - Configuration management for production and/or base hosts (logins, ntpd, repo RPMs, etc.)\n+    - Determine upgrade procedure: do we upgrade the hosts in place with the option of reverting to production images or\n+      do we hotswap production machines with freshly installed testing machines\n+\n+\n+## Configuration\n+\n+Basic host configuration is handled by Ansible and a local Git repository of playbooks.\n+\n+### Git Repository\n+\n+The authoritative Git repository for Madison ITB configuration is `gitolite@git.chtc.wisc.edu:osgitb`.  Clone the\n+repository and push validated changes back to it.\n+\n+### Ansible\n+\n+The `osghost` machine has Ansible 2.3.1.0 installed via RPM.  Use other hosts and versions at your own risk.\n+\n+#### Common Ansible commands\n+\n+**Note:**\n+\n+- For critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person\n+- All commands below are meant to be run from the `osgitb` directory from Git\n+\n+To run Ansible for the first time on a new machine (using the `root` password when prompted):\n+\n+    :::console\n+    ansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN\n+    ansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN\n+\n+The `HOST-PATTERN` can be a glob-like pattern or a regular expression that matches host names in the inventory file; see\n+Ansible documentation for details.\n+\n+After initial successful runs of both playbooks, subsequent runs should replace the `-u root -k` part with `-bK` to use\n+your own login and `sudo`.  For example:\n+\n+    :::console\n+    ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN\n+    ansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ]\n+\n+Omit the `-l` option to apply configuration to all hosts.\n+\n+If you have your own playbook to manage personal configuration, run it as follows:\n+\n+    :::console\n+    ansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ]\n+\n+#### Adding host and other certificates\n+\n+(This is in very rough form, but the key bits are here.)\n+\n+1. Ask Mat to get new certificates\u00a0\u2014 be sure to think about `http`, `rsv`, and other service certificates\n+2. Wait for Mat to tell you that the new certificates are in `/p/condor/home/certificates`\n+3. `scp -p` the certificate(s) (`*cert.pem*` and `*key.pem`) to your home directory on `osghost` or whatever machine you use for Ansible\n+4. Find the corresponding certificate location(s) in the Ansible `roles/certs/files` directory\n+5. `cp -p` the certificate files over the top of the existing Ansible ones (or create new, equivalent paths)\n+6. Run `ansible-vault encrypt FILE(S)` to encrypt the files\u00a0\u2014 get the Ansible vault password from Tim\u00a0C. if you need it\n+7. Verify permissions, contents (you can `cat` the encrypted files), etc.\n+8. Apply the files with something like `ansible-playbook secure.yml -i inventory -bK -f 20 -t certs`\n+9. Commit changes (now or after applying)\n+10. Push changes to origin\n+\n+#### Doing yum updates\n+\n+1. Check to see if updates are needed and, if so, what would be updated:\n+\n+        :::console\n+        ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a 'yum check-update'\n+\n+    You can name a single `HOST` or an inventory `GROUP` (such as the handy `current` group); with a group, you can\n+    further restrict the hosts with a `-l` option.\n+\n+    **Note:** `yum check-update` exits with status code `100` when it succeeds in identifying packages to update;\n+    therefore Ansible shows such results as failures.\n+\n+2. Review the package lists to be updated and decide whether to proceed with all updates or limited ones\n+\n+3. Do updates:\n+\n+        :::console\n+        ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a 'yum --assumeyes update' [ -l LIMITS ]\n+\n+#### Updating HTCondor from Upcoming\n+\n+Something like this:\n+\n+    :::console\n+    ansible condordev -i inventory -bK -f 10 -m command -a 'yum --enablerepo=osg-upcoming --assumeyes update condor'\n+\n+\n+## Monitoring\n+\n+### HTCondor-CE View\n+\n+Once we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available\n+[here](http://itb-ce1.chtc.wisc.edu:59619) through HTCondor-CE View.\n+\n+### Tracking payload jobs via Kibana\n+\n+At this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via <literal><a href=\"<https://gracc.opensciencegrid.org/kibana/app/kibana#/discover?_g>=(refreshInterval:(display:Off,pause:f,value:0),time:(from:now%2Fw,mode:quick,to:now%2Fw))&\\_a=(columns:!(\\_source),index:%27gracc.osg-itb.raw-\\*%27,interval:auto,query:(query\\_string:(analyze\\_wildcard:!f,query:%27\\*%27)),sort:!(EndTime,desc))\">Kibana</a></literal>. To view all payload jobs that have run on our ITB site in the past week, use <literal><a href=\"<https://gracc.opensciencegrid.org/kibana/app/kibana#/discover?_g>=(refreshInterval:(display:Off,pause:f,value:0),time:(from:now%2Fw,mode:quick,to:now%2Fw))&\\_a=(columns:!(ProbeName),index:%27gracc.osg-itb.raw-\\*%27,interval:auto,query:(query\\_string:(analyze\\_wildcard:!f,query:%27ResourceType:%20Payload%20AND%20ProbeConfig:%20%22condor:itb-ce1.chtc.wisc.edu%22%27)),sort:!(EndTime,desc))\">this query</a></literal>.\n+\n+\n+## Making a New Virtual Machine on itb-host-1\n+\n+For this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins.\n+If you do not have an account, request one from the CHTC system administrators.\n+\n+1. If this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure\n+    1. Pick a MAC address, starting with `00:16:3e:` followed by three random octets (e.g., `00:16:3e:f7:29:ee`)\n+    1. Email <htcondor-inf@cs.wisc.edu> with a request for a new OSG ITB VM, including the chosen MAC address\n+    1. Wait to receive the associated IP address for the new host\n+\n+1. Create or edit the Cobbler system object for the host\n+    1. Access <https://cobbler-widmir.chtc.wisc.edu/cobbler_web>\n+    1. In the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link\n+    1. If desired, filter (at the bottom) by \u201cname\u201d on something like `itb-*.chtc.wisc.edu`\n+    1. For a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d\n+    1. For a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry\n+    1. In the **first** \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific\\_6\\_8\\_osg\\_vm\u201d or \u201cScientific\\_7\\_2\\_osg\\_vm\u201d\n+    1. In the **second** \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox\n+    1. In the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine\n+    1. In the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host\n+    1. Click the \u201cSave\u201d button\n+    1. In the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link\n+\n+1. Log in to `itb-host-1` and become `root`\n+\n+1. Create the libvirt definition file\n+    1. Create a new XML file named after the desired hostname (e.g., `itb-ce2.xml`) and copy in the template below\n+    1. Replace `{{ HOSTNAME }}` with the fully qualified hostname of the new virtual host\n+    1. Replace `{{ MAC_ADDRESS }}` with the MAC address of the new virtual host (from above)\n+    1. If desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed\n+    1. Save the XML file\n+    1. Create a new, empty disk image for the virtual host, in its correct location (as specified in the XML file):\n+\n+            :::console\n+            truncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd\n+            chown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd\n+\n+    1. Load the new host definition into libvirt:\n+\n+            :::console\n+            virsh define XML-FILE\n+\n+1. Install the new machine\n+    1. Start the virtual machine:\n+\n+            :::console\n+            virsh start HOSTNAME\n+\n+        At this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS,\n+        then reboot the now-installed machine.  The whole process typically takes 15\u201320 minutes.  You may be able to\n+        `ssh` into the machine during the install process, but there is no need to monitor or interfere.\n+\n+    1. Once the machine is available (which you can only guess at), `ssh` in and verify that the machine basically works\n+    1. Immediately run Ansible on the machine, first with the `secure.yml` playbook, then the `site.yml` one (see above)\n+    1. Log in to the machine and look around to make sure it seems OK\n+    1. When things look good, tell virsh to start the virtual machine when the host itself starts:\n+\n+            :::console\n+            virsh autostart HOSTNAME\n+\n+### Libvirt VM Template\n+\n+``` xml\n+<domain type='kvm'>\n+\n+  <name>{{ HOSTNAME }}</name>\n+\n+  <memory unit='GiB'>6</memory>\n+  <vcpu>4</vcpu>\n+  <os>\n+    <type>hvm</type>\n+    <boot dev='network'/>\n+    <boot dev='hd'/>\n+    <bios useserial='yes' rebootTimeout='0'/>\n+  </os>\n+  <features>\n+    <acpi/>\n+    <apic/>\n+    <pae/>\n+  </features>\n+\n+  <devices>\n+\n+    <emulator>/usr/libexec/qemu-kvm</emulator>\n+\n+    <disk type='file' device='disk'>\n+      <source file='/var/lib/libvirt/images/{{ HOSTNAME }}.dd'/>\n+      <target dev='vda' bus='virtio'/>\n+    </disk>\n+\n+    <interface type='bridge'>\n+      <mac address='{{ MAC_ADDRESS }}'/>\n+      <source bridge='br0'/>\n+      <model type='virtio'/>\n+    </interface>\n+\n+    <serial type='pty'>\n+      <target port='0'/>\n+    </serial>\n+    <console type='pty'>\n+      <target type='serial' port='0'/>\n+    </console>\n+\n+    <graphics type='vnc' autoport='yes' listen='127.0.0.1'/>\n+\n+  </devices>\n+</domain>\n+```\n+\n+## Archive", 
  "html_url": "https://github.com/opensciencegrid/technology/pull/307#discussion_r139491103", 
  "id": 139491103, 
  "original_commit_id": "527f6897ca67f147ea6bd45556db202fdfed48ab", 
  "original_position": 279, 
  "path": "docs/infrastructure/madison-itb.md", 
  "position": null, 
  "pull_request_review_id": 63431018, 
  "pull_request_url": "https://api.github.com/repos/opensciencegrid/technology/pulls/307", 
  "updated_at": "2017-09-18T17:49:42Z", 
  "url": "https://api.github.com/repos/opensciencegrid/technology/pulls/comments/139491103", 
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/390105?v=4", 
    "events_url": "https://api.github.com/users/brianhlin/events{/privacy}", 
    "followers_url": "https://api.github.com/users/brianhlin/followers", 
    "following_url": "https://api.github.com/users/brianhlin/following{/other_user}", 
    "gists_url": "https://api.github.com/users/brianhlin/gists{/gist_id}", 
    "gravatar_id": "", 
    "html_url": "https://github.com/brianhlin", 
    "id": 390105, 
    "login": "brianhlin", 
    "organizations_url": "https://api.github.com/users/brianhlin/orgs", 
    "received_events_url": "https://api.github.com/users/brianhlin/received_events", 
    "repos_url": "https://api.github.com/users/brianhlin/repos", 
    "site_admin": false, 
    "starred_url": "https://api.github.com/users/brianhlin/starred{/owner}{/repo}", 
    "subscriptions_url": "https://api.github.com/users/brianhlin/subscriptions", 
    "type": "User", 
    "url": "https://api.github.com/users/brianhlin"
  }
}
